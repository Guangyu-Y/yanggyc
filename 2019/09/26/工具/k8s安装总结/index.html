<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Guangyu Yang">





<title>K8S安装总结 | Hexo</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 5.4.2"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Yanggyc&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Yanggyc&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">K8S安装总结</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Guangyu Yang</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">September 26, 2019&nbsp;&nbsp;12:44:48</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/%E4%BA%91%E5%8E%9F%E7%94%9F/">云原生</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <p><code>官方资料：https://feisky.gitbooks.io/kubernetes/components/kubeadm.html</code><br><code>[Kubernetes] 国内获取 Kubernetes 镜像的方法</code><br><code>https://blog.csdn.net/shida_csdn/article/details/78480241</code><br><code>https://www.cnblogs.com/fengzhihai/p/9849683.html</code></p>
<h4 id="Rancher方式："><a href="#Rancher方式：" class="headerlink" title="Rancher方式："></a>Rancher方式：</h4><p>二次封装的常用发行版【可视化构建kubernetes】</p>
<ol>
<li>minikube方式：</li>
</ol>
<p>Minikube是一个工具，可以在本地快速运行一个单点的Kubernetes，适合尝试Kubernetes或日常开发的用户使用，但是不能用于生产环境。</p>
<ol start="2">
<li>kubeadm方式：<br>Kubeadm也是一个工具，提供kubeadm init和kubeadm join，可用于快速部署Kubernetes集群。<br>二进制包方式：<br>从官方下载发行版的二进制包，手动部署每个组件，组成Kubernetes集群，过程较为繁琐。</li>
</ol>
<p>本例使用kubeadmin工具安装。<br>安装要求：<br>1、centos7.X<br>2、2G、2C、30GB<br>3、集群之间网络互通<br>4、可以访问外网，需要拉去镜像</p>
<p>部署步骤：<br>0、设置先设条件<br>1、在所有节点上安装Doctor和kubeadm、kubelet<br>2、部署kubernetes的Master<br>3、部署容器网络插件<br>4、部署kubernetes Node，将节点加入到kubernetes集群中<br>5、部署Dashboard Web页面，可视化查看Kubernetes资源</p>
<p>先设条件【所有节点】：<br>1、关闭防火墙【影响通信，学习，也可以将默认的规则清空重新生成】<br>systemctl stop firewalld<br>systemctl disable firewalld<br>systemctl status firewalld</p>
<p>2、关闭selinux：linux下的一个安全机制，主要是对文件系统访问做一个权限控制，这个权限控制会影响到kubernetes中的一个组件kuberlete的安装，因为这个组件的安装会访问本地的文件操作系统<br>setenforce 0<br>vim /etc/selinux/config ：# 更改为 SELINUX=disabled<br>getenforce:快照查询</p>
<p>3、禁止swap分区：swap分区的作用是当物理内存不足时，利用swap分区做数据交换，但是在kubernetes中完全不支持swap分区，所以必须禁止掉，或者创建系统的时候就不创建他<br>    临时关闭：swapoff -a<br>    永久性关闭: vim /etc/fstab<br>    中注释掉：/dev/mapper/centos-swap swap     swap    defaults        0 0<br>sysctl –system<br>free -m</p>
<p>4、添加主机名和对应ip的关系<br>ip1    主机名1<br>ip2    主机名2<br>ip3    主机名3</p>
<p>5、时间同步【启动chronyd系统服务】<br>systemctl  start  chronyd.service<br>systemctl  enable  chronyd.service</p>
<p>6、将桥接的IPv4流量传递到iptables的链【有一些ipv4的流量不能走iptables链【linux内核的一个过滤器，每个流量都会经过他，然后再匹配是否可进入当前应用进程去处理】，导致流量丢失】：<br>配置k8s.conf文件（#k8s.conf文件原来不存在，需要自己创建的）<br>cat /etc/sysctl.d/k8s.conf<br>vim /etc/sysctl.d/k8s.conf<br>net.bridge.bridge-nf-call-ip6tables=1<br>net.bridge.bridge-nf-call-iptables=1<br>vm.swappiness=0</p>
<p>sysctl –system</p>
<p>4、系统路由参数,防止kubeadm报路由警告（所有结点）<br>echo “<br>net.bridge.bridge-nf-call-ip6tables = 1<br>net.bridge.bridge-nf-call-iptables = 1<br>“ &gt;&gt; /etc/sysctl.conf<br>sysctl -p</p>
<p>安装安装Doctor、kubeadm【引导集群的客户端工具】、kubelet【kubernetes中管理容器】【所有节点】<br>1、安装Docker<br>配置Docker官方源并将下载的源文件存放于/etc/yum.repos.d/下：<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo">https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</a> -o /etc/yum.repos.d/docker-ce.repo</p>
<p>安装：yum -y install docker-ce-18.06.3.ce-3.el7.x86_64<br>卸载：sudo yum remove docker<br>yum list installed|grep docker<br>rpm -qa|grep docker<br>yum –y remove docker.x86_64<br>rm -rf /var/lib/docker<br>dokcer </p>
<p>设置开机启动并启动docker：systemctl enable docker &amp;&amp; systemctl start docker</p>
<p>2、安装kubeadm、kubelet、kubectl<br>配置kubernetes的阿里YUM软件源： 放在 /etc/yum.repos.d 目录下，vim /etc/yum.repos.d/kubernetes.repo<br>[kubernetes]<br>name=Kubernetes<br>baseurl=<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/">https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/</a><br>enabled=1<br>gpgcheck=1<br>repo_gpgcheck=1<br>gpgkey=<a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg">https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg</a> <a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg">https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg</a><br>安装：<br>yum install -y kubelet-1.13.3<br>yum install -y kubeadm-1.13.3<br>yum install -y kubectl-1.13.3<br>systemctl enable kubelet【此时此刻不需要启动kubelet，因为最后kubeadm会一键部署，仅仅设置开机启动即可】</p>
<p>部署【初始化】kubernetes Master【Master节点】<br>kubeadm init <br>–apiserver-advertise-address 192.168.192.129 <br>–image-repository registry.aliyuncs.com/google_containers <br>–kubernetes-version=v1.15.2 <br>–service-cidr=10.1.0.0/16 <br>–pod-network-cidr=10.244.0.0/16</p>
<p>kubeadm init –apiserver-advertise-address 172.23.62.32 –image-repository registry.aliyuncs.com/google_containers –kubernetes-version=v1.13.3 –service-cidr=10.1.0.0/16 –pod-network-cidr=10.244.0.0/16</p>
<p>kubeadm init <br>–apiserver-advertise-address 172.23.62.32 <br>–image-repository registry.aliyuncs.com/google_containers <br>–kubernetes-version=v1.13.3 <br>–pod-network-cidr=192.168.0.0/16</p>
<p>kubeadm init –apiserver-advertise-address 172.23.62.32 –image-repository registry.aliyuncs.com/google_containers –kubernetes-version=v1.13.3 –pod-network-cidr=192.168.0.0/16</p>
<p>这里没有设置–service-cidr 是因为我们下面需要部署calico网络，<br>calico会帮助我们设置service网络，如果此地设置了service网络会导致calico部署不成功<br>kubeadm join 192.168.106.129:6443 –token vh7r6w.xlvzl3u6tijqvenu –discovery-token-ca-cert-hash sha256:da0d3f50910a6fb56efd3a31f389feeb9757fe3ee242b7c114379db0037c80dd</p>
<p>beta.kubernetes.io/os: linux<br>kubectl label nodes admin beta.kubernetes.io/os=linux</p>
<p>kubectl get node -a -l “beta.kubernetes.io/os=linux”</p>
<p>使用kubectl工具，执行：<br>To start using your cluster, you need to run the following as a regular user:</p>
<p>  mkdir -p $HOME/.kube<br>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br>  sudo chown $(id -u):$(id -g) $HOME/.kube/config</p>
<p>You should now deploy a pod network to the cluster.<br>Run “kubectl apply -f [podnetwork].yaml” with one of the options listed at:<br>  <a target="_blank" rel="noopener" href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">https://kubernetes.io/docs/concepts/cluster-administration/addons/</a></p>
<p>You can now join any number of machines by running the following on each node<br>as root:</p>
<p>  kubeadm join 10.7.20.42:6443 –token us3rfx.78zk5wqh0bwzgy0v –discovery-token-ca-cert-hash sha256:f1ed3b3f874f4eeb236ba02f46c33332a127db951d308406075c3e1fc01e89ce</p>
<p>kubectl get node</p>
<p>记住：<br>You can now join any number of machines by running the following on each node<br>as root:<br>kubeadm join 10.7.20.42:6443 –token us3rfx.78zk5wqh0bwzgy0v –discovery-token-ca-cert-hash sha256:f1ed3b3f874f4eeb236ba02f46c33332a127db951d308406075c3e1fc01e89ce</p>
<p>安装Pod网络插件【master节点，node节点加入后自动下载】<br>kubectl apply -f <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml">https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml</a><br>查看状况：<br>kubectl get pods -n kube-system<br>kubectl get node</p>
<p>将node节点加入网络【node节点执行】：<br>token获取：<br>kubeadm token list<br>kubeadm token create<br>discovery-token-ca-cert-hash sha256获取：<br>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed ‘s/^.* //‘</p>
<p>kubeadm join 192.168.88.156:6443 –token z5zhnh.rbqaolxsfm44aotw –discovery-token-ca-cert-hash sha256:e618a5e9356666a01ff5d607c0683147a5b4cab85d4b88bbce571bb272482983</p>
<p>kubeadm join 10.7.19.116:6443 –token ot6jtu.50tduzh1erfwjvqe –discovery-token-ca-cert-hash sha256:947900641657188f7e1bc0f02fad5ab5796f409d635eb194b5062e7db1aa225b</p>
<p>kubeadm join 172.23.62.32:6443 –token rswamh.wbpmjvpuqj5x422c –discovery-token-ca-cert-hash sha256:ee7a60db8a8ab1e45106f55498c4429e7374612e85262a62853641c9379c54d3</p>
<p>kubeadm join 10.7.19.116:6443 –tokengacdq2.m4pfvg4qboijck23 –discovery-token-ca-cert-hash sha256:8ae0b26a736e4c4ce98f952f1f0a2e462c32994e1e462c40c2ce9726a0c1a206</p>
<p>kubectl get node</p>
<p>举例：创建一个nginx的pod<br>kubectl create deployment nginx –image=nginx<br>创建service：暴露容器让外部访问【–type=NodePort说明创建的是NodePort类型，则通过任意一个NodeIP+Port就可以访问】：<br>kubectl expose deployment nginx –port=80 –type=NodePort<br>查看pod详情：kubectl get pods,svc -o wide<br>192.168.88.151:30344<br>192.168.88.152:30344<br>192.168.88.153:30344</p>
<p>kubernetes-dashboard安装<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_41806245/article/details/89381752">https://blog.csdn.net/weixin_41806245/article/details/89381752</a><br>[Kubernetes] 国内获取 Kubernetes 镜像的方法<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/shida_csdn/article/details/78480241">https://blog.csdn.net/shida_csdn/article/details/78480241</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/fengzhihai/p/9849683.html">https://www.cnblogs.com/fengzhihai/p/9849683.html</a><br>主要参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/java_zyq/article/details/82178152">https://blog.csdn.net/java_zyq/article/details/82178152</a><br>验证：kubectl get pods -n kube-system<br>kubernetes的webui配置：dashboard<br>1、下载安装yaml文件：wget <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml">https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</a><br>2、装备镜像<br>3、修改安装文件<br>需要给外部暴露端口所以：</p>
<h1 id="——————-Dashboard-Service-——————"><a href="#——————-Dashboard-Service-——————" class="headerlink" title="——————- Dashboard Service ——————-"></a>——————- Dashboard Service ——————-</h1><p>spec:<br>  type: NodePort<br>  ports:<br>    - port: 443<br>      targetPort: 8443<br>      nodePort: 30001 # 指定对外端口<br>  selector:<br>    k8s-app: kubernetes-dashboard</p>
<p>因为网络问题需要提前自行装备镜像并修改：</p>
<h1 id="——————-Dashboard-Deployment-——————"><a href="#——————-Dashboard-Deployment-——————" class="headerlink" title="——————- Dashboard Deployment ——————-"></a>——————- Dashboard Deployment ——————-</h1><p>image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3</p>
<p>4、创建Dashboard<br>kubectl apply -y kubernetes-dashboard.yaml<br>等待查看[kubectl get pods,svc -o wide -n kube-system]dashboard处于running状态后即可访问。<br>此处坑多：尤其是镜像问题。<br>1、配置镜像和仓库已经下载的镜像对应不上而导致脚本自行下载不下来而出错。<br>2、配置成功后仅仅可以在部署节点利用https+ip+端口方式访问，不能在宿主机访问。<br>3、进入后登陆方式待后续。</p>
<p>5、访问Dashboard：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above">https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above</a><br>访问有如下三种方式：<br>1、NodePort【节点主机访问】<br>至此就可以使用NodePort访问Dashboard。可以在部署节点利用https+ip+端口方式访问，不能在宿主机访问。<br>2、kubectl proxy【宿主机】最简单的方式就是开启代理方式可以在宿主机访问：<br>在Master上执行kubecll proxy，然后使用如下地址访问Dashboard：<br><a target="_blank" rel="noopener" href="http://192.168.88.151:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy">http://192.168.88.151:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a><br>但限制就是必须在Master上访问，这显然是个坑，我们的目标是在我们真实的物理机上去访问Master的Dashboard。</p>
<p>所以，在主节点上，我们执行<br>kubectl proxy –address=192.168.88.156 –disable-filter=true<br>kubectl proxy –address=0.0.0.0 –port=8080 –disable-filter=true<br>kubectl proxy –address=0.0.0.0 –port=8080 –disable-filter=true<br><a target="_blank" rel="noopener" href="http://172.23.62.32:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy">http://172.23.62.32:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a><br><a target="_blank" rel="noopener" href="http://10.7.19.116:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy">http://10.7.19.116:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a><br><a target="_blank" rel="noopener" href="http://192.168.192.129:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy">http://192.168.192.129:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a><br>开启代理。<br>显示如下说明代理开启：<br>W0515 09:16:46.963911   27314 proxy.go:140] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious<br>Starting to serve on 192.168.88.151:8001<br>其中：<br>address表示外界可以使用192.168.88.151来访问Dashboard，我们也可以使用0.0.0.0<br>disable-filter=true表示禁用请求过滤功能，否则我们的请求会被拒绝，并提示 Forbidden (403) Unauthorized。<br>我们也可以指定端口，具体请查看kubectl proxy –help<br>3、apiserver<br>4、Ingress</p>
<p>6、登陆Dashboard<br>A、token方式登陆：<br>创建SA，并绑定默认cluster-admin【k8s的超级管理员】管理员集群角色<br>kubectl create serviceaccount dashboard-admin -n kube-system<br>kubectl create clusterrolebinding dashboard-admin –clusterrole=cluster-admin –serviceaccount=kube-system:dashboard-admin<br>查看刚刚创建dashboard-admin的token信息<br>kubectl get secret -n kube-system<br>kubectl describe secret dashboard-admin-token-sprhf -n kube-system<br>B、点击跳过使用默认方式登陆：<br>此时方式登陆之后整个页面提示：很遗憾，我们看到了很多权限错误提示，主要是system:serviceaccount:kube-system:kubernetes-dashboard的权限不足引起的。<br>因为权限的问题，所以需要在kubernetes-dashboard.yaml中对默认登陆用户设置更高权限的角色，<br>因此，我们可以更改RoleBinding修改为ClusterRoleBinding，并且修改roleRef中的kind和name，<br>使用cluster-admin这个非常牛逼的CusterRole（超级用户权限，其拥有访问kube-apiserver的所有权限）。如下：<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: ClusterRoleBinding<br>metadata:<br>  name: kubernetes-dashboard-minimal<br>  namespace: kube-system<br>roleRef:<br>  apiGroup: rbac.authorization.k8s.io<br>  kind: ClusterRole<br>  name: cluster-admin<br>subjects:</p>
<ul>
<li>kind: ServiceAccount<br>name: kubernetes-dashboard<br>namespace: kube-system</li>
</ul>
<p>当主节点ip发生变化时，重新构建集群步骤：<br>1、修改hosts文件【各节点】<br>2、kubeadm reset 或者 参考：<a target="_blank" rel="noopener" href="https://www.colabug.com/596892.html">https://www.colabug.com/596892.html</a><br>3、删除 /root/.kube/ 下文件和目录<br>4、docker ps 查看<br>5、修改–apiserver-advertise-address参数 重新构建<br>kubeadm init <br>–apiserver-advertise-address 192.168.88.156 <br>–image-repository registry.aliyuncs.com/google_containers <br>–kubernetes-version=v1.13.3 <br>–service-cidr=10.1.0.0/16 <br>–pod-network-cidr=10.244.0.0/16<br>后续一样，使用kubectl工具、安装pod网络kube-flannel、加入节点</p>
<p>当子节点ip发生变化时，重新构建集群步骤：<br>1、修改hosts文件【各节点】<br>2、kubeadm reset<br>3、执行更新后的：kubeadm join 192.168.88.153:6443 –token nrd1eg.xzb1h6nkkazjt5fg –discovery-token-ca-cert-hash sha256:73c8b0c56493929ed8d2a5b7641110df9187641455415eb6240569026bd460c8<br>4、可能会报错：The connection to the server localhost:8080 was refused - did you specify the right host or port?<br>解决方法如下：<br>1、将主节点中的【/etc/kubernetes/admin.conf】文件拷贝到从节点相同目录下<br>2、配置环境变量<br>echo “export KUBECONFIG=/etc/kubernetes/admin.conf” &gt;&gt; ~/.bash_profile<br>立即生效：source ~/.bash_profile</p>
<p>问题：子节点执行kubectl get pods报错：<br>The connection to the server localhost:8080 was refused - did you specify the right host or port?<br>出现这个问题的原因是kubectl命令需要使用kubernetes-admin来运行。<br>解决方法如下：<br>1、将主节点中的【/etc/kubernetes/admin.conf】文件拷贝到从节点相同目录下<br>2、配置环境变量<br>echo “export KUBECONFIG=/etc/kubernetes/admin.conf” &gt;&gt; ~/.bash_profile<br>立即生效：source ~/.bash_profile<br>参考：<a target="_blank" rel="noopener" href="https://www.jianshu.com/p/6fa06b9bbf6a">https://www.jianshu.com/p/6fa06b9bbf6a</a></p>
<p>kubeadm 续坑篇<br><a target="_blank" rel="noopener" href="https://www.colabug.com/596892.html">https://www.colabug.com/596892.html</a><br><a target="_blank" rel="noopener" href="https://mritd.me/2016/11/21/kubeadm-other-problems/">https://mritd.me/2016/11/21/kubeadm-other-problems/</a></p>
<p>kubeadm常用命令：</p>
<p>kubectl：常用操作命令：<br>创建：kubectl run <name> –image=<image> 或者 kubectl create -f manifest.yaml<br>查询：kubectl get <resource><br>更新 kubectl set 或者 kubectl patch<br>删除：kubectl delete <resource> <name> 或者 kubectl delete -f manifest.yaml<br>查询 Pod IP：kubectl get pod <pod-name> -o jsonpath=’{.status.podIP}’<br>查看状况：<br>kubectl get pods -n kube-system<br>kubectl get node<br>查看pod详情：<br>kubectl get pods,svc -o wide<br>kubectl get node<br>查看在哪个节点上：kubectl get pods -o wide<br>查看指定命名空间：kube-system下：kubectl get pods -n kube-system<br>查看所有命名空间：kubectl get pod –all-namespaces</p>
<p>kubectl操作节点相关命令：<br>列出各个node及其基本信息：kubectl get nodes<br>列出各个node及其全部基本信息：kubectl get nodes<br>查询某个节点的Event信息：kubectl describe node 节点名称<k8snode01><br>查询某个pod的Event信息：kubectl describe pod pod名称<mysql-6r6fw></p>
<p>kubernetes的动态扩缩<br>1、修改RC的副本数量<br>2、动态扩缩：kubectl scale rc &lt;镜像名称&gt;  –replicas=1</p>
<p>容器内执行命令：kubectl exec -ti <pod-name> sh<br>容器日志：kubectl logs [-f] <pod-name><br>导出服务：kubectl expose deploy <name> –port=80<br>kubectl logs 用于显示 pod 运行中，容器内程序输出到标准输出的内容。跟 docker 的 logs 命令类：kubectl logs nginx<br>kubectl attach 用于连接到一个正在运行的容器。跟 docker 的 attach 命令类似<br>kubectl exec 用于在一个正在运行的容器执行命令。跟 docker 的 exec 命令类似。<br>kubectl exec 123456-7890 date<br>Options:<br>  -c, –container=’’: Container name. If omitted, the first container in the pod will be chosen<br>  -p, –pod=’’: Pod name<br>  -i, –stdin=false: Pass stdin to the container<br>  -t, –tty=false: Stdin is a TT</p>
<p>kubectl cp 支持从容器中拷贝，或者拷贝文件到容器中<br><a target="_blank" rel="noopener" href="https://feisky.gitbooks.io/kubernetes/components/kubectl.html">https://feisky.gitbooks.io/kubernetes/components/kubectl.html</a></p>
<p>待解决问题1：<br>kubectl logs podname 不起作用<br>升级内核并配置：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zcc_heu/article/details/78822551">https://blog.csdn.net/zcc_heu/article/details/78822551</a></p>
<p>kubernetes的webui配置：dashboard<br>1、下载安装yaml文件：wget <a target="_blank" rel="noopener" href="https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml">https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</a><br>2、装备镜像<br>3、修改安装文件<br>需要给外部暴露端口所以：</p>
<h1 id="——————-Dashboard-Service-——————-1"><a href="#——————-Dashboard-Service-——————-1" class="headerlink" title="——————- Dashboard Service ——————-"></a>——————- Dashboard Service ——————-</h1><p>spec:<br>  type: NodePort<br>  ports:<br>    - port: 443<br>      targetPort: 8443<br>      nodePort: 30001 # 指定对外端口<br>  selector:<br>    k8s-app: kubernetes-dashboard</p>
<p>因为网络问题需要提前自行装备镜像并修改：</p>
<h1 id="——————-Dashboard-Deployment-——————-1"><a href="#——————-Dashboard-Deployment-——————-1" class="headerlink" title="——————- Dashboard Deployment ——————-"></a>——————- Dashboard Deployment ——————-</h1><p>image: k8s.gcr.io/kubernetes-dashboard-amd64:v1.8.3</p>
<p>4、创建Dashboard<br>kubectl apply -y kubernetes-dashboard.yaml<br>等待查看[kubectl get pods,svc -o wide -n kube-system]dashboard处于running状态后即可访问。<br>此处坑多：尤其是镜像问题。<br>1、配置镜像和仓库已经下载的镜像对应不上而导致脚本自行下载不下来而出错。<br>2、配置成功后仅仅可以在部署节点利用https+ip+端口方式访问，不能在宿主机访问。<br>3、进入后登陆方式待后续。</p>
<p>5、访问Dashboard：<a target="_blank" rel="noopener" href="https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above">https://github.com/kubernetes/dashboard/wiki/Accessing-Dashboard---1.7.X-and-above</a><br>访问有如下三种方式：<br>1、NodePort【节点主机访问】<br>至此就可以使用NodePort访问Dashboard。可以在部署节点利用https+ip+端口方式访问，不能在宿主机访问。<br>2、kubectl proxy【宿主机】最简单的方式就是开启代理方式可以在宿主机访问：<br>在Master上执行kubecll proxy，然后使用如下地址访问Dashboard：<br><a target="_blank" rel="noopener" href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy</a><br>但限制就是必须在Master上访问，这显然是个坑，我们的目标是在我们真实的物理机上去访问Master的Dashboard。</p>
<p>所以，在主节点上，我们执行kubectl proxy –address=192.168.56.101 –disable-filter=true开启代理。<br>显示如下说明代理开启：<br>W0515 09:16:46.963911   27314 proxy.go:140] Request filter disabled, your proxy is vulnerable to XSRF attacks, please be cautious<br>Starting to serve on 192.168.88.151:8001<br>其中：<br>address表示外界可以使用192.168.56.101来访问Dashboard，我们也可以使用0.0.0.0<br>disable-filter=true表示禁用请求过滤功能，否则我们的请求会被拒绝，并提示 Forbidden (403) Unauthorized。<br>我们也可以指定端口，具体请查看kubectl proxy –help<br>3、apiserver<br>4、Ingress</p>
<p>6、登陆Dashboard<br>A、token方式登陆：<br>创建SA，并绑定默认cluster-admin【k8s的超级管理员】管理员集群角色<br>kubectl create serviceaccount dashboard-admin -n kube-system<br>kubectl create clusterrolebinding dashboard-admin –clusterrole=cluster-admin –serviceaccount=kube-system:dashboard-admin<br>查看刚刚创建dashboard-admin的token信息<br>kubectl get secret -n kube-system<br>kubectl describe secret dashboard-admin-token-sprhf -n kube-system<br>B、点击跳过使用默认方式登陆：<br>此时方式登陆之后整个页面提示：很遗憾，我们看到了很多权限错误提示，主要是system:serviceaccount:kube-system:kubernetes-dashboard的权限不足引起的。<br>因为权限的问题，所以需要在kubernetes-dashboard.yaml中对默认登陆用户设置更高权限的角色，<br>因此，我们可以更改RoleBinding修改为ClusterRoleBinding，并且修改roleRef中的kind和name，<br>使用cluster-admin这个非常牛逼的CusterRole（超级用户权限，其拥有访问kube-apiserver的所有权限）。如下：<br>apiVersion: rbac.authorization.k8s.io/v1<br>kind: ClusterRoleBinding<br>metadata:<br>  name: kubernetes-dashboard-minimal<br>  namespace: kube-system<br>roleRef:<br>  apiGroup: rbac.authorization.k8s.io<br>  kind: ClusterRole<br>  name: cluster-admin<br>subjects:</p>
<ul>
<li>kind: ServiceAccount<br>name: kubernetes-dashboard<br>namespace: kube-system</li>
</ul>
<p>搭建K8S Ingress：<a target="_blank" rel="noopener" href="https://blog.csdn.net/java_zyq/article/details/82179107">https://blog.csdn.net/java_zyq/article/details/82179107</a></p>
<p>至此</p>
<p>1、创建Dashboard资料<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/java_zyq/article/details/82178152%E3%80%81">https://blog.csdn.net/java_zyq/article/details/82178152、</a></p>
<p><a target="_blank" rel="noopener" href="http://172.23.62.32:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login">http://172.23.62.32:8080/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/#!/login</a></p>
<p>kubectl proxy –address=0.0.0.0 –disable-filter=true</p>
<p>谷歌浏览器</p>
<p>2、访问Dashboard</p>
<p>创建SA，并绑定默认cluster-admin【k8s的超级管理员】管理员集群角色<br>kubectl create serviceaccount dashboard-admin -n kube-system<br>kubectl create clusterrolebinding dashboard-admin –clusterrole=cluster-admin –serviceaccount=kube-system:dashboard-admin</p>
<p>查看刚刚创建dashboard-admin的token信息<br>kubectl get secret -n kube-system<br>kubectl describe secret dashboard-admin-token-sprhf -n kube-system</p>
<p>宿主机虚拟机ip变化解决<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/x356982611/article/details/82260987">https://blog.csdn.net/x356982611/article/details/82260987</a><br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/wycc/p/6066342.html">https://www.cnblogs.com/wycc/p/6066342.html</a><br><a target="_blank" rel="noopener" href="https://wenku.baidu.com/view/412d9650302b3169a45177232f60ddccdb38e65a.html">https://wenku.baidu.com/view/412d9650302b3169a45177232f60ddccdb38e65a.html</a><br>先去掉net的dhcp，修改配置，重启网络，添加上net的dhcp勾选</p>
<p>将docker镜像导入导出<br>docker save -o nginx.tar nginx:latest<br>docker load -i nginx.tar </p>
<p>整个配置文件不需要添加【服务，pod，控制】名称即可。<br>获取pod指定的配置文件<br>kubectl get po -o yaml<br>kubectl get pod -o yaml<br>获取pod的整个定义文件<br>kubectl get po kubia-zxzij -o yaml<br>kubectl get pod kubia-zxzij -o yaml<br>获取service整个定义文件<br>kubectl get svc -o yaml<br>kubectl get service -o yaml<br>获取service指定定义文件<br>kubectl get svc service-name -o yaml<br>kubectl get service service-name -o yaml<br>获取controller整个定义文件<br>kubectl get rc -o yaml<br>kubectl get replicationcontroller -o yaml<br>获取controller指定定义文件<br>kubectl get rc rc-name -o yaml<br>kubectl get replicationcontroller rc-name -o yaml</p>
<p>查看各种【po/svc/rc】资源yaml定义文件的配置属性说明：<br>kubectl explain pod/po<br>kubectl explain service/svc<br>kubectl explain replicationcontroller/rc<br>查看各种【po/svc/rc】资源yaml定义文件的具体配置属性说明：<br>kubectl explain pod/po.kind<br>kubectl explain service/svc.spec<br>kubectl explain replicationcontroller/rc.metadata</p>
<p>查询端口<br>ETCDCTL_API=3 etcdctl del /registry/pods/default/auth-7dd856444-c5lkk<br>强制删除：kubectl delete pod auth-7dd856444-c5lkk –force –grace-period=0</p>
<hr>
<p><em><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>NFS</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></em>**********************************************************</p>
<hr>
<p>一个小demo：<br><a target="_blank" rel="noopener" href="https://www.cnblogs.com/atcloud/p/10614326.html">https://www.cnblogs.com/atcloud/p/10614326.html</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/johnhuang-cn/spring-cloud-k8s-ci-template">https://github.com/johnhuang-cn/spring-cloud-k8s-ci-template</a></p>
<p>查看pod环境变量：kubectl exec eureka-6bd8dcfff4-ht7jf env<br>pipeline</p>
<p>nfs server定义：<br>1、关闭防火墙【影响通信，学习，也可以将默认的规则清空重新生成】<br>systemctl stop firewalld<br>systemctl disable firewalld<br>systemctl status firewalld<br>每一个节点安装nfs相关程序包：<br>yum -y install nfs-utils<br>mount<br>vim /etc/exports 配置导出目录<br>/data/volumes 192.168.88.120/180(rw,no_root_squash)<br>启动服务：systemctl start nfs<br>查看启动程序：ss -tal<br>查看监听端口：ss -tnl<br>在node2上执行：mount -t nfs k8snfs:/data/volumes /mnt</p>
<p>注意：在启动任何一个NFS之前，必须做好端口和功能的对应映射工作，这个工作由rpcbind完成；<br>一、服务端环境搭建<br>1、nfs-utils：NFS服务主程序；<br>  yum install -y nfs-utils<br>nfs-utils-1.3.0-0.61.el7.x86_64<br> 2、rpcbind服务；<br>  yum install -y rpcbind<br>rpcbind-0.2.0-47.el7.x86_64<br> 3、启动顺序：先开启rpcbind，再开启nfs服务；<br>  systemctl start rpcbind.service<br>  systemctl start nfs-server.service</p>
<p>  4、设置开机自启动；<br>  systemctl enable rpcbind.service</p>
<p>或者：</p>
<p> ( ln -s /usr/lib/systemd/system/rpcbind.service /etc/systemd/system/multi-user.target.wants/rpcbind.service)</p>
<p>  systemctl enable nfs-server.service</p>
<p>5、关闭防火墙并关闭开启自启动；<br>  systemctl stop firewalld<br>  systemctl disable firewalld<br>  6、编辑/etc/exports,并平滑重启nfs服务；<br>  /data 192.168.1.0/24(rw,sync)</p>
<p>  systemctl reload nfs-server.service</p>
<p>7、设置共享目录属主、权限；<br>  chmod 777 /data<br>  chown -R nfsnobody.nfsnobody /data</p>
<p>二、客户端环境搭建；<br>1、rpcbind服务；<br>  yum install -y rpcbind</p>
<p>  2、开启rpcbind服务，并设置开机自启动；<br>  systemctl start rpcbind.service</p>
<p>  systemctl enable rpcbind.service</p>
<p>  3、关闭防火墙并关闭开机自启动；<br>  systemctl stop firewalld</p>
<p>  systemctl disable firewalld</p>
<p>  4、查看nfs共享，并挂载；<br>  showmount -e nfs服务器IP：共享目录</p>
<p>  mount -t nfs nfs服务器IP：/data /mnt</p>
<h2 id="data-volumes-rw-sync-insecure-no-subtree-check-no-root-squash"><a href="#data-volumes-rw-sync-insecure-no-subtree-check-no-root-squash" class="headerlink" title="/data/volumes *(rw,sync,insecure,no_subtree_check,no_root_squash)"></a>/data/volumes *(rw,sync,insecure,no_subtree_check,no_root_squash)</h2><p>作者：牛逼哄哄__<br>来源：CSDN<br>原文：<a target="_blank" rel="noopener" href="https://blog.csdn.net/zsycsnd/article/details/79737349">https://blog.csdn.net/zsycsnd/article/details/79737349</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p>
<p>问题：<br>showmount -e报错<br>[root@node1 mnt]# showmount -e<br>clnt_create: RPC: Unknown host</p>
<p>NFS mount errors with “clnt_create: RPC: Unknown host” for CentOS 6</p>
<h1 id="showmount-e"><a href="#showmount-e" class="headerlink" title="showmount -e"></a>showmount -e</h1><p>clnt_create: RPC: Unknown host  </p>
<p>解决：<a target="_blank" rel="noopener" href="https://linuxcluster.wordpress.com/2014/08/19/nfs-mount-errors-with-clnt_create-rpc-unknown-host-for-centos-6/">https://linuxcluster.wordpress.com/2014/08/19/nfs-mount-errors-with-clnt_create-rpc-unknown-host-for-centos-6/</a><br>service nfs restart</p>
<p>测试</p>
<h1 id="showmount-e-localhost"><a href="#showmount-e-localhost" class="headerlink" title="showmount -e localhost"></a>showmount -e localhost</h1><p>Export list for localhost:  </p>
<hr>
<p><em><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>HPA</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></em>**********************************************************</p>
<hr>
<p>/etc/kubernetes/manifests/kube-controller-manager.yaml<br>添加：- –horizontal-pod-autoscaler-use-rest-clients=false</p>
<hr>
<p><em><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>STORAGECLASS</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></em>*******************************************</p>
<hr>
<hr>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>master关闭网络联系管理员操作后的问题</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>*******************************************************************************************************************</p>
<hr>
<hr>
<p>现象：pod不能删除一直处于Terminating状态，服务不能访问，dashboard不能访问，新建pod不能成功，远程桌面不能登录。<br>A、pod不能删除一直处于Terminating状态，但是查看子节点的容器已经删除。<br>直观解决：强制删除：kubectl delete pod auth-7dd856444-c5lkk –force –grace-period=0<br>本质原因：nfs服务未开启，开启nfs服务<br> 3、启动顺序：先开启rpcbind，再开启nfs服务；<br>  systemctl start rpcbind.service<br>  systemctl start nfs-server.service<br>  4、设置开机自启动；<br>  systemctl enable rpcbind.service<br>B、服务不能访问，dashboard不能访问，新建pod不能成功<br>nfs服务未启动<br>C、远程桌面不能登录<br>lsof -i:5901 查看服务，为输出任何信息说明未启动服务。<br>解决方法：启动远程登录服务<br>后台启动：vncserver<br>临时启动：startx</p>
<hr>
<hr>
<pre><code>                                    kubernetes 平台卸载
</code></pre>
<hr>
<hr>
<hr>
<hr>
<p>master：<br>1、rpm -q -a | grep kube<br>2、rpm -e kubeadm-1.13.3-0.x86_64 kubelet-1.13.3-0.x86_64 kubectl-1.14.3-0.x86_64 kubernetes-cni-0.6.0-0.x86_64<br>rm -rf /etc/kubernetes/<br>rm -rf /var/lib/etcd<br>rm -rf /var/lib/kubelet/<br>rm -rf $HOME/.kube<br>systemctl stop kubelet.service<br>9、kill -9 8939 【kube-prox】<br>10、停止所有运行容器，删除想·</p>
<p>node：<br>1、rpm -q -a | grep kube<br>2、rpm -e kubeadm-1.13.3-0.x86_64 kubelet-1.13.3-0.x86_64 kubectl-1.14.3-0.x86_64 kubernetes-cni-0.6.0-0.x86_64<br>rpm -e kubeadm-1.13.3-0.x86_64 kubelet-1.13.3-0.x86_64 kubectl-1.14.3-0.x86_64 kubernetes-cni-0.6.0-0.x86_64<br>rm -rf /etc/kubernetes/<br>rm -rf /var/lib/kubelet/<br>rm -rf /var/lib/etcd/</p>
<p>Linux无法删除文件夹 Device or resource busy<br>umount /jenkins2/docker/plugins 再删除<br><a target="_blank" rel="noopener" href="https://segmentfault.com/a/1190000017086124">https://segmentfault.com/a/1190000017086124</a><br>kubeadm reset<br>systemctl stop kubelet<br>systemctl stop docker<br>rm -rf /var/lib/cni/<br>rm -rf /var/lib/kubelet/*<br>rm -rf /etc/cni/<br>rm -rf /etc/cni/net.d/flannel<br>ifconfig cni0 down<br>ifconfig flannel.1 down<br>ifconfig docker0 down<br>ip link delete cni0<br>ip link delete flannel.1<br>systemctl start docker</p>
<p>kubeadm token list<br>kubeadm token create<br>discovery-token-ca-cert-hash sha256获取：<br>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed ‘s/^.* //‘</p>
<p>kubeadm join 172.23.62.32:6443 –token e0l8mw.t15m8uriym9d7h71 –discovery-token-ca-cert-hash sha256:ee7a60db8a8ab1e45106f55498c4429e7374612e85262a62853641c9379c54d3</p>
<p>docker rm <code>docker ps -a | grep Exited | awk &#39;&#123;print $1&#125;&#39;</code>  </p>
<p>kubeadm init <br>–apiserver-advertise-address 172.23.62.32 <br>–image-repository registry.aliyuncs.com/google_containers <br>–kubernetes-version=v1.13.3 <br>–service-cidr=10.1.0.0/16 <br>–pod-network-cidr=10.244.0.0/16</p>
<p>kubeadm init <br>–apiserver-advertise-address 172.23.62.32 <br>–image-repository registry.aliyuncs.com/google_containers <br>–kubernetes-version=v1.13.3 <br>–service-cidr=10.1.0.0/16 <br>–pod-network-cidr=10.244.0.0/16 <br>–dry-run</p>
<p>kubeadm join 172.23.62.32:6443 –token wzazwa.y1lcc7suap887coh –discovery-token-ca-cert-hash sha256:3346b204579dfe414fc42a17ff62e428cca364171c4e837b75c08fa40db74c4b</p>
<p>args:</p>
<pre><code>    - --apiserver-host=http://192.168.80.130:8080
</code></pre>
<p>kubeadm init <br>–apiserver-advertise-address 172.23.62.32 <br>–image-repository registry.aliyuncs.com/google_containers <br>–kubernetes-version=v1.13.3 <br>–pod-network-cidr=10.244.0.0/16</p>
<p>将node节点加入网络【node节点执行】：<br>token获取：<br>kubeadm token list<br>kubeadm token create<br>discovery-token-ca-cert-hash sha256获取：<br>openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed ‘s/^.* //‘</p>
<p>kubeadm join 192.168.88.156:6443 –token z5zhnh.rbqaolxsfm44aotw –discovery-token-ca-cert-hash sha256:e618a5e9356666a01ff5d607c0683147a5b4cab85d4b88bbce571bb272482983</p>
<p>kubeadm join 10.7.19.116:6443 –token ot6jtu.50tduzh1erfwjvqe –discovery-token-ca-cert-hash sha256:947900641657188f7e1bc0f02fad5ab5796f409d635eb194b5062e7db1aa225b</p>
<p>kubeadm join 10.7.19.116:6443 –token 6na7bo.805ugrsh8gtbsxnv –discovery-token-ca-cert-hash sha256:947900641657188f7e1bc0f02fad5ab5796f409d635eb194b5062e7db1aa225b</p>
<p>kubectl get node</p>
<hr>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>linux多节点免密互通</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>**************************************<br><a target="_blank" rel="noopener" href="https://blog.csdn.net/xinhui88/article/details/17585537">https://blog.csdn.net/xinhui88/article/details/17585537</a></p>
<hr>
<p>SSH互信设置步骤：</p>
<ol>
<li>每个节点上分别生成自己的公钥和私钥</li>
<li>将各节点的公钥文件汇总到一个总的认证文件authorized_keys中</li>
<li>将这个包含了所有节点公钥的认证文件authorized_keys分发到各个节点中去</li>
<li>验证节点间互信</li>
<li>每个节点上分别生成自己的公钥和私钥，并把公钥放到本机的authorized_keys中<br>rac1:<br>su - oracle<br>ssh-keygen -t rsa<br>ssh-keygen -t dsa<br>cd .ssh<br>cat *.pub &gt; authorized_keys</li>
</ol>
<p>rac2:<br>su - oracle<br>ssh-keygen -t rsa<br>ssh-keygen -t dsa<br>cd .ssh<br>cat *.pub &gt; authorized_keys</p>
<ol start="2">
<li><p>将各节点的公钥文件汇总到一个总的认证文件authorized_keys中<br>rac1:<br>scp /root/.ssh/authorized_keys rac2:/root/authorized_keys_rac1<br>cat /home/oracle/authorized_keys_rac1 &gt;&gt; /home/oracle/.ssh/authorized_keys</p>
</li>
<li><p>将这个包含了所有节点公钥的认证文件authorized_keys分发到各个节点中去</p>
</li>
</ol>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/yunweiadmin/p/10441724.html">https://www.cnblogs.com/yunweiadmin/p/10441724.html</a><br>node节点pod无法启动/节点删除网络重置<br>node1之前反复添加过,添加之前需要清除下网络<br>root@master1:/var/lib/kubelet# kubectl get po -o wide<br>NAME READY STATUS RESTARTS AGE IP NODE<br>nginx-8586cf59-6zw9k 1/1 Running 0 9m 10.244.3.3 node2<br>nginx-8586cf59-jk5pc 0/1 ContainerCreating 0 9m <none> node1<br>nginx-8586cf59-vm9h4 0/1 ContainerCreating 0 9m <none> node1<br>nginx-8586cf59-zjb84 1/1 Running 0 9m 10.244.3.2 node2<br>root@node1:~# journalctl -u kubelet<br>failed: rpc error: code = Unknown desc = NetworkPlugin cni failed to set up pod “nginx-8586cf59-rm4sh_default” network: failed to set bridge addr: “cni0” already has an IP address different from 10.244.2.1/24<br>12252 cni.go:227] Error while adding to cni network: failed to set bridge addr: “cni0” already<br>重置kubernetes服务，重置网络。删除网络配置，link<br>kubeadm reset<br>systemctl stop kubelet<br>systemctl stop docker<br>rm -rf /var/lib/cni/<br>rm -rf /var/lib/kubelet/*<br>rm -rf /etc/cni/<br>ifconfig cni0 down<br>ifconfig flannel.1 down<br>ifconfig docker0 down<br>ip link delete cni0<br>ip link delete flannel.1<br>systemctl start docker</p>
<hr>
<p><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong><strong>kubernetest离线安装</strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong></strong>*************************************</p>
<hr>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Guangyu Yang</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://www.yanggyc.work/2019/09/26/%E5%B7%A5%E5%85%B7/k8s%E5%AE%89%E8%A3%85%E6%80%BB%E7%BB%93/">http://www.yanggyc.work/2019/09/26/%E5%B7%A5%E5%85%B7/k8s%E5%AE%89%E8%A3%85%E6%80%BB%E7%BB%93/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>Do you believe in <strong>DESTINY</strong>?</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/%E5%A4%87%E5%BF%98/"># 备忘</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2019/09/26/%E5%B7%A5%E5%85%B7/%E4%B8%80%E4%B8%AApython%E7%9A%84%E6%96%87%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%84%9A%E6%9C%AC/">python的文件服务脚本</a>
            
            
            <a class="next" rel="next" href="/2019/09/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/">hadoop</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Guangyu Yang | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>