<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.2">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.1/css/all.min.css" integrity="sha256-Z1K5uhUaJXA7Ll0XrZ/0JhX4lAtZFpT6jkKrEDT0drU=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.yanggyc.work","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.14.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="Hadoop第一章 大数据的概述1.1 大数据的概念最近几年，IT行业最火的名词中，少不了”大数据”、”人工智能”、”云计算”、”物联网”、”区块链”等等这些名词。针对于**”大数据”**这个名词，现在更是全国老百姓，老少皆知的一个词语。但是什么是大数据，除了IT行业的专业人士外，其他人乃至其他行业的人，除了能说出”数据量大”之外，好像真的不能再更深层次的解释了。那么我们来看看下面几个权威机构给出">
<meta property="og:type" content="article">
<meta property="og:title" content="hadoop">
<meta property="og:url" content="http://www.yanggyc.work/2019/09/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/index.html">
<meta property="og:site_name" content="Yanggyc.Blog">
<meta property="og:description" content="Hadoop第一章 大数据的概述1.1 大数据的概念最近几年，IT行业最火的名词中，少不了”大数据”、”人工智能”、”云计算”、”物联网”、”区块链”等等这些名词。针对于**”大数据”**这个名词，现在更是全国老百姓，老少皆知的一个词语。但是什么是大数据，除了IT行业的专业人士外，其他人乃至其他行业的人，除了能说出”数据量大”之外，好像真的不能再更深层次的解释了。那么我们来看看下面几个权威机构给出">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401012832988.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401013100075.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401013159672.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104441770.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104504725.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104625494.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104650560.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104811173.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401105627870.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401105639009.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401105654266.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110221175.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110453904.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110508586.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110748741.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110919469.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401111009995.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401111303037.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401111326294.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401112649301.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210414093345557.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210414093407057.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210414095815391.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/20191016063209.jpg">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210414100109033.png">
<meta property="og:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210414101324462.png">
<meta property="article:published_time" content="2019-09-26T04:44:48.000Z">
<meta property="article:modified_time" content="2022-12-27T07:15:05.766Z">
<meta property="article:author" content="Guangyu Yang">
<meta property="article:tag" content="概念">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://notebook-1257935960.file.myqcloud.com//img/image-20210401012832988.png">


<link rel="canonical" href="http://www.yanggyc.work/2019/09/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://www.yanggyc.work/2019/09/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/","path":"2019/09/26/大数据开发/Hadoop/","title":"hadoop"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>hadoop | Yanggyc.Blog</title>
  






  <script async defer data-website-id="" src=""></script>

  <script defer data-domain="" src=""></script>

  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Yanggyc.Blog</p>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="/uploads/custom-logo.png" alt="Yanggyc.Blog">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-schedule"><a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Hadoop"><span class="nav-number">1.</span> <span class="nav-text">Hadoop</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="nav-number">1.1.</span> <span class="nav-text">第一章 大数据的概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-number">1.1.1.</span> <span class="nav-text">1.1 大数据的概念</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E7%89%B9%E5%BE%81"><span class="nav-number">1.1.2.</span> <span class="nav-text">1.2 大数据的特征</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">1.1.3.</span> <span class="nav-text">1.3 大数据的应用场景</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8F%91%E5%B1%95%E5%89%8D%E6%99%AF"><span class="nav-number">1.1.4.</span> <span class="nav-text">1.4 大数据的发展前景</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-Hadoop%E6%A6%82%E8%BF%B0"><span class="nav-number">1.2.</span> <span class="nav-text">第二章 Hadoop概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E7%94%A8hadoop"><span class="nav-number">1.2.1.</span> <span class="nav-text">2.1 为什么要用hadoop</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Hadoop%E7%9A%84%E7%AE%80%E8%A6%81%E4%BB%8B%E7%BB%8D"><span class="nav-number">1.2.2.</span> <span class="nav-text">2.2 Hadoop的简要介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-%E8%B0%B7%E6%AD%8C%E7%9A%84%E4%B8%89%E7%AF%87%E8%AE%BA%E6%96%87"><span class="nav-number">1.2.3.</span> <span class="nav-text">2.3 谷歌的三篇论文</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Hadoop%E7%9A%84%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2"><span class="nav-number">1.2.4.</span> <span class="nav-text">2.4 Hadoop的发展历史</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Hadoop%E7%9A%84%E7%BB%84%E6%88%90%E9%83%A8%E5%88%86"><span class="nav-number">1.2.5.</span> <span class="nav-text">2.5 Hadoop的组成部分</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-Hadoop%E7%9A%84%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F"><span class="nav-number">1.2.6.</span> <span class="nav-text">2.6. Hadoop的生态系统</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0-Hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85"><span class="nav-number">1.3.</span> <span class="nav-text">第三章 Hadoop集群安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 集群规划</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-%E5%AE%89%E8%A3%85JDK"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 安装JDK</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-%E6%A3%80%E6%9F%A5%E4%B8%80%E4%B8%8B%E6%98%AF%E5%90%A6%E5%B7%B2%E7%BB%8F%E5%AE%89%E8%A3%85%E8%BF%87%E6%88%96%E8%80%85%E7%B3%BB%E7%BB%9F%E5%86%85%E7%BD%AEJDK-%E5%A6%82%E6%9E%9C%E6%9C%89%E5%86%85%E7%BD%AE%E7%9A%84%EF%BC%8C%E5%B0%86%E5%85%B6%E5%8D%B8%E8%BD%BD"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">3.2.1 检查一下是否已经安装过或者系统内置JDK,如果有内置的，将其卸载</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-%E4%B8%8A%E4%BC%A0jdk1-8"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">3.2.2 上传jdk1.8</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-%E8%A7%A3%E5%8E%8Bjdk%E5%88%B0-usr-local-%E4%B8%8B"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">3.2.3 解压jdk到&#x2F;usr&#x2F;local&#x2F;下</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-4-%E6%9B%B4%E5%90%8Djdk"><span class="nav-number">1.3.2.4.</span> <span class="nav-text">3.2.4 更名jdk</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-5-%E9%85%8D%E7%BD%AEJdk%E7%9A%84%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%EF%BC%9A-etc-profile"><span class="nav-number">1.3.2.5.</span> <span class="nav-text">3.2.5 配置Jdk的环境变量：&#x2F;etc&#x2F;profile</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-6-%E4%BD%BF%E5%BD%93%E5%89%8D%E7%AA%97%E5%8F%A3%E7%94%9F%E6%95%88"><span class="nav-number">1.3.2.6.</span> <span class="nav-text">3.2.6 使当前窗口生效</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-7-%E9%AA%8C%E8%AF%81jdk%E7%8E%AF%E5%A2%83"><span class="nav-number">1.3.2.7.</span> <span class="nav-text">3.2.7 验证jdk环境</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E7%8E%AF%E5%A2%83%E9%9C%80%E6%B1%82%E5%8F%8A%E5%AE%89%E8%A3%85"><span class="nav-number">1.3.3.</span> <span class="nav-text">3.3. 完全分布式环境需求及安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-1-%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">3.3.1 关闭防火墙</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-2-%E9%9D%99%E6%80%81IP%E5%92%8C%E4%B8%BB%E6%9C%BA%E5%90%8D%E9%85%8D%E7%BD%AE"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">3.3.2 静态IP和主机名配置</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-3-%E9%85%8D%E7%BD%AE-etc-hosts%E6%96%87%E4%BB%B6"><span class="nav-number">1.3.3.3.</span> <span class="nav-text">3.3.3 配置&#x2F;etc&#x2F;hosts文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-4-%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95%E8%AE%A4%E8%AF%81"><span class="nav-number">1.3.3.4.</span> <span class="nav-text">3.3.4 免密登录认证</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-5-%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="nav-number">1.3.3.5.</span> <span class="nav-text">3.3.5 时间同步</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-3-6-Hadoop%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E9%85%8D%E7%BD%AE"><span class="nav-number">1.3.3.6.</span> <span class="nav-text">3.3.6 Hadoop安装与环境变量配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Hadoop%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-number">1.3.4.</span> <span class="nav-text">3.4. Hadoop的配置文件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-1-%E6%A6%82%E8%BF%B0"><span class="nav-number">1.3.4.1.</span> <span class="nav-text">3.4.1. 概述</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-2-core-site-xml"><span class="nav-number">1.3.4.2.</span> <span class="nav-text">3.4.2. core-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-3-hdfs-site-xml"><span class="nav-number">1.3.4.3.</span> <span class="nav-text">3.4.3. hdfs-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-4-mapred-site-xml"><span class="nav-number">1.3.4.4.</span> <span class="nav-text">3.4.4. mapred-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-5-yarn-site-xml"><span class="nav-number">1.3.4.5.</span> <span class="nav-text">3.4.5 yarn-site.xml</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-6-hadoop-env-sh"><span class="nav-number">1.3.4.6.</span> <span class="nav-text">3.4.6 hadoop-env.sh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-7-yarn-env-sh"><span class="nav-number">1.3.4.7.</span> <span class="nav-text">3.4.7 yarn-env.sh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-8-slaves"><span class="nav-number">1.3.4.8.</span> <span class="nav-text">3.4.8 slaves</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-4-9-%E5%88%86%E5%8F%91%E5%88%B0%E5%8F%A6%E5%A4%96%E4%B8%A4%E5%8F%B0%E8%8A%82%E7%82%B9"><span class="nav-number">1.3.4.9.</span> <span class="nav-text">3.4.9 分发到另外两台节点</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-%E6%A0%BC%E5%BC%8F%E5%8C%96%E4%B8%8E%E5%90%AF%E5%8A%A8"><span class="nav-number">1.3.5.</span> <span class="nav-text">3.5 格式化与启动</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-1-%E6%A0%BC%E5%BC%8F%E5%8C%96%E9%9B%86%E7%BE%A4"><span class="nav-number">1.3.5.1.</span> <span class="nav-text">3.5.1 格式化集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-5-2-%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">1.3.5.2.</span> <span class="nav-text">3.5.2 启动集群</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-HDFS%E7%9A%84Shell%E5%91%BD%E4%BB%A4"><span class="nav-number">1.4.</span> <span class="nav-text">第四章 HDFS的Shell命令</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#4-1-%E5%88%9B%E5%BB%BA%E7%9B%AE%E5%BD%95"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 创建目录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-2-%E4%B8%8A%E4%BC%A0%E6%8C%87%E4%BB%A4"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2 上传指令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-3-%E5%88%9B%E5%BB%BA%E7%A9%BA%E6%96%87%E4%BB%B6"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3 创建空文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-4-%E5%90%91%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E9%87%8C%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9"><span class="nav-number">1.4.4.</span> <span class="nav-text">4.4 向分布式文件系统中的文件里追加内容</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-5-%E6%9F%A5%E7%9C%8B%E6%8C%87%E4%BB%A4"><span class="nav-number">1.4.5.</span> <span class="nav-text">4.5 查看指令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-6-%E4%B8%8B%E8%BD%BD%E6%8C%87%E4%BB%A4"><span class="nav-number">1.4.6.</span> <span class="nav-text">4.6 下载指令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-7-%E5%90%88%E5%B9%B6%E4%B8%8B%E8%BD%BD"><span class="nav-number">1.4.7.</span> <span class="nav-text">4.7 合并下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-8-%E7%A7%BB%E5%8A%A8hdfs%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%EF%BC%88%E6%9B%B4%E5%90%8D%EF%BC%89"><span class="nav-number">1.4.8.</span> <span class="nav-text">4.8  移动hdfs中的文件（更名）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-9-%E5%A4%8D%E5%88%B6hdfs%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6%E5%88%B0hdfs%E7%9A%84%E5%8F%A6%E4%B8%80%E4%B8%AA%E7%9B%AE%E5%BD%95"><span class="nav-number">1.4.9.</span> <span class="nav-text">4.9 复制hdfs中的文件到hdfs的另一个目录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-10-%E5%88%A0%E9%99%A4%E5%91%BD%E4%BB%A4"><span class="nav-number">1.4.10.</span> <span class="nav-text">4.10 删除命令</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-11-%E6%9F%A5%E7%9C%8B%E7%A3%81%E7%9B%98%E5%88%A9%E7%94%A8%E7%8E%87%E5%92%8C%E6%96%87%E4%BB%B6%E5%A4%A7%E5%B0%8F"><span class="nav-number">1.4.11.</span> <span class="nav-text">4.11 查看磁盘利用率和文件大小</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-12-%E4%BF%AE%E6%94%B9%E6%9D%83%E9%99%90%E7%9A%84"><span class="nav-number">1.4.12.</span> <span class="nav-text">4.12 修改权限的</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-13-%E4%BF%AE%E6%94%B9%E6%96%87%E4%BB%B6%E7%9A%84%E5%89%AF%E6%9C%AC%E6%95%B0"><span class="nav-number">1.4.13.</span> <span class="nav-text">4.13 修改文件的副本数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-14-%E6%9F%A5%E7%9C%8B%E6%96%87%E4%BB%B6%E7%9A%84%E7%8A%B6%E6%80%81"><span class="nav-number">1.4.14.</span> <span class="nav-text">4.14 查看文件的状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-15-%E6%B5%8B%E8%AF%95"><span class="nav-number">1.4.15.</span> <span class="nav-text">4.15 测试</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%BA%94%E7%AB%A0-HDFS%E7%9A%84%E5%9D%97%E7%9A%84%E6%A6%82%E5%BF%B5"><span class="nav-number">1.5.</span> <span class="nav-text">第五章 HDFS的块的概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#5-1-%E4%BC%A0%E7%BB%9F%E5%9E%8B%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1 传统型分布式文件系统的缺点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-2-HDFS%E7%9A%84%E5%9D%97"><span class="nav-number">1.5.2.</span> <span class="nav-text">5.2 HDFS的块</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-3-HDFS%E7%9A%84%E5%9D%97%E5%A4%A7%E5%B0%8F"><span class="nav-number">1.5.3.</span> <span class="nav-text">5.3 HDFS的块大小</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-4-%E5%9D%97%E7%9A%84%E7%9B%B8%E5%85%B3%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.5.4.</span> <span class="nav-text">5.4 块的相关参数设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-5-%E5%9D%97%E7%9A%84%E5%AD%98%E5%82%A8%E4%BD%8D%E7%BD%AE"><span class="nav-number">1.5.5.</span> <span class="nav-text">5.5 块的存储位置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-6-HDFS%E7%9A%84%E4%BC%98%E7%82%B9"><span class="nav-number">1.5.6.</span> <span class="nav-text">5.6 HDFS的优点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5-7-HDFS%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-number">1.5.7.</span> <span class="nav-text">5.7 HDFS的缺点</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AD%E7%AB%A0-HDFS%E7%9A%84%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84"><span class="nav-number">1.6.</span> <span class="nav-text">第六章 HDFS的体系结构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E8%A7%A3%E6%9E%90"><span class="nav-number">1.6.1.</span> <span class="nav-text">6.1 体系结构解析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-%E5%BC%80%E6%9C%BA%E5%90%AF%E5%8A%A8HDFS%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="nav-number">1.6.2.</span> <span class="nav-text">6.2 开机启动HDFS的过程</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-%E9%9D%9E%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">1.6.2.1.</span> <span class="nav-text">6.2.1 非第一次启动集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%90%AF%E5%8A%A8%E9%9B%86%E7%BE%A4"><span class="nav-number">1.6.2.2.</span> <span class="nav-text">6.2.2 第一次启动集群</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-SecondaryNameNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6"><span class="nav-number">1.6.3.</span> <span class="nav-text">6.3 SecondaryNameNode的工作机制</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B8%83%E7%AB%A0-HDFS%E7%9A%84%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B"><span class="nav-number">1.7.</span> <span class="nav-text">第七章 HDFS的读写流程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-%E8%AF%BB%E6%B5%81%E7%A8%8B%E8%AF%A6%E8%A7%A3"><span class="nav-number">1.7.1.</span> <span class="nav-text">7.1 读流程详解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-%E5%86%99%E6%B5%81%E7%A8%8B%E7%9A%84%E8%AF%A6%E8%A7%A3"><span class="nav-number">1.7.2.</span> <span class="nav-text">7.2 写流程的详解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%85%AB%E7%AB%A0-Zookeeper%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="nav-number">1.8.</span> <span class="nav-text">第八章 Zookeeper的概述</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#8-1-Zookeeper%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.8.1.</span> <span class="nav-text">8.1 Zookeeper是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-2-Zookeeper%E7%9A%84%E7%89%B9%E7%82%B9"><span class="nav-number">1.8.2.</span> <span class="nav-text">8.2 Zookeeper的特点</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-3-Zookeeper%E7%9A%84%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.8.3.</span> <span class="nav-text">8.3 Zookeeper的数据模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#8-4-Zookeeper%E7%9A%84%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="nav-number">1.8.4.</span> <span class="nav-text">8.4 Zookeeper的应用场景</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E4%B9%9D%E7%AB%A0-Zookeeper%E7%9A%84%E5%AE%89%E8%A3%85"><span class="nav-number">1.9.</span> <span class="nav-text">第九章 Zookeeper的安装</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#9-1-%E5%AE%89%E8%A3%85%E4%B8%8E%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">1.9.1.</span> <span class="nav-text">9.1. 安装与环境变量的配置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#9-2-%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">1.9.2.</span> <span class="nav-text">9.2. 集群模式的配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-1-Zookeeper%E7%9A%84%E6%9C%8D%E5%8A%A1%E8%BF%9B%E7%A8%8B%E5%B8%83%E5%B1%80"><span class="nav-number">1.9.2.1.</span> <span class="nav-text">9.2.1 Zookeeper的服务进程布局</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-2-%E4%BF%AE%E6%94%B9zoo-cfg%E6%96%87%E4%BB%B6"><span class="nav-number">1.9.2.2.</span> <span class="nav-text">9.2.2 修改zoo.cfg文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-3-%E6%B7%BB%E5%8A%A0myid"><span class="nav-number">1.9.2.3.</span> <span class="nav-text">9.2.3 添加myid</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-4-%E6%90%AD%E5%BB%BA%E5%85%B6%E4%BB%96%E4%B8%A4%E4%B8%AAserver%E8%8A%82%E7%82%B9%E7%9A%84%E7%8E%AF%E5%A2%83"><span class="nav-number">1.9.2.4.</span> <span class="nav-text">9.2.4 搭建其他两个server节点的环境</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#9-2-5-%E5%90%AF%E5%8A%A8zookeeper"><span class="nav-number">1.9.2.5.</span> <span class="nav-text">9.2.5 启动zookeeper</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E7%AB%A0-Zookeeper%E7%9A%84Shell%E6%93%8D%E4%BD%9C"><span class="nav-number">1.10.</span> <span class="nav-text">第十章 Zookeeper的Shell操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%80%E7%AB%A0-YARN%E7%9A%84%E6%A6%82%E8%BF%B0"><span class="nav-number">1.11.</span> <span class="nav-text">第十一章 YARN的概述</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%8C%E7%AB%A0-YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%8F%8A%E7%BB%84%E4%BB%B6"><span class="nav-number">1.12.</span> <span class="nav-text">第十二章 YARN的架构及组件</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#12-1-MapReduce-1-x%E7%9A%84%E7%AE%80%E4%BB%8B"><span class="nav-number">1.12.1.</span> <span class="nav-text">12.1. MapReduce 1.x的简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-2-YARN%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3"><span class="nav-number">1.12.2.</span> <span class="nav-text">12.2. YARN的设计思想</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#12-3-YARN%E7%9A%84%E9%85%8D%E7%BD%AE"><span class="nav-number">1.12.3.</span> <span class="nav-text">12.3. YARN的配置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%B8%89%E7%AB%A0-YARN%E7%9A%84%E6%89%A7%E8%A1%8C%E5%8E%9F%E7%90%86"><span class="nav-number">1.13.</span> <span class="nav-text">第十三章 YARN的执行原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E5%9B%9B%E7%AB%A0-YARN%E7%9A%84%E6%A1%88%E4%BE%8B%E6%B5%8B%E8%AF%95"><span class="nav-number">1.14.</span> <span class="nav-text">第十四章 YARN的案例测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AC%AC%E5%8D%81%E4%BA%94%E7%AB%A0-YARN%E7%9A%84Web-UI%E6%9F%A5%E7%9C%8B"><span class="nav-number">1.15.</span> <span class="nav-text">第十五章 YARN的Web UI查看</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Guangyu Yang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">61</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">12</span>
        <span class="site-state-item-name">categories</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://www.yanggyc.work/2019/09/26/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/Hadoop/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Guangyu Yang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yanggyc.Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="hadoop | Yanggyc.Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          hadoop
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2019-09-26 12:44:48" itemprop="dateCreated datePublished" datetime="2019-09-26T12:44:48+08:00">2019-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2022-12-27 15:15:05" itemprop="dateModified" datetime="2022-12-27T15:15:05+08:00">2022-12-27</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91/" itemprop="url" rel="index"><span itemprop="name">大数据开发</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><h2 id="第一章-大数据的概述"><a href="#第一章-大数据的概述" class="headerlink" title="第一章 大数据的概述"></a>第一章 大数据的概述</h2><h3 id="1-1-大数据的概念"><a href="#1-1-大数据的概念" class="headerlink" title="1.1 大数据的概念"></a>1.1 大数据的概念</h3><p>最近几年，IT行业最火的名词中，少不了”大数据”、”人工智能”、”云计算”、”物联网”、”区块链”等等这些名词。针对于**”大数据”**这个名词，现在更是全国老百姓，老少皆知的一个词语。但是什么是大数据，除了IT行业的专业人士外，其他人乃至其他行业的人，除了能说出”数据量大”之外，好像真的不能再更深层次的解释了。那么我们来看看下面几个权威机构给出的解释。</p>
<p>**维基百科 **给出的定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">数据规模巨大到无法通过人工在合理的时间内达到截取，管理，处理并整理成为人类所解读的信息。</span><br></pre></td></tr></table></figure>

<p>**麦肯锡全球研究所 **给出的定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一种规模大到在获取、存储、管理、分析方面都大大超出了传统数据库软件工具能力范围的数据集合。</span><br></pre></td></tr></table></figure>

<p>**研究机构 **高德纳(Gartner)给出的定义：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;大数据&quot;是需要新的处理模式才能具有更强的决策力、洞察发现力和流程优化能力来适应海量、高增长率和多样化的信息资产</span><br></pre></td></tr></table></figure>

<p><strong>概念总结：</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">海量数据，具有高增长率、数据类型多样化、一定时间内无法使用常规软件工具进行捕捉、管理和处理的数据集合。 </span><br></pre></td></tr></table></figure>



<h3 id="1-2-大数据的特征"><a href="#1-2-大数据的特征" class="headerlink" title="1.2 大数据的特征"></a>1.2 大数据的特征</h3><p>早在1980年，著名未来学家托夫勒在其所著的《第三次浪潮》中就热情地将“大数据”称颂为“第三次浪潮的华彩乐章”。《自然》杂志在2008年9月推出了名为“大数据”的封面专栏。从2009年开始“大数据”才成为互联网技术行业中的热门词汇。最早应用“大数据”的是世界著名的管理咨询公司麦肯锡公司，它看到了各种网络平台记录的个人海量信息具备潜在的商业价值，于是投入大量人力物力进行调研， 对“大数据”进行收集和分析的设想，在2011年6月发布了关于“大数据”的报告，该报告对“大数据”的影响、关键技术和应用领域等都进行了详尽的分析。麦肯锡的报告得到了金融界的高度重视，而后逐渐受到了各行各业关注。 那么大数据到底有什么特征呢？我们怎么去理解大数据呢？有专业人士总结了4V说法，也有相关机构总结了5V说法，甚至6V说法。不管哪种说法，下面四个特征，是大家普遍认可的。</p>
<ul>
<li><strong>Volume</strong> : 巨大的数据量</li>
<li><strong>Variety</strong> : 数据类型多样化<ul>
<li>结构化的数据 : 即具有固定格式和有限长度的数据</li>
<li>半结构化的数据 : 是一些xml或者html格式的数据</li>
<li>非结构化的数据 : 现在非结构化的数据越来越多，就是不定长、无固定格式的数据，例如网页、语音、视频等</li>
</ul>
</li>
<li><strong>Velocity</strong> : 数据增长速度快</li>
<li><strong>Value</strong> : 价值密度低，商业价值高</li>
</ul>
<h3 id="1-3-大数据的应用场景"><a href="#1-3-大数据的应用场景" class="headerlink" title="1.3 大数据的应用场景"></a>1.3 大数据的应用场景</h3><p>有不了解大数据的人会问：<strong>大数据能做啥？</strong>问的好。</p>
<p>大数据本身是一个抽象的概念， 对当前无论是企业还是政府、或是高校等单位来说，是一个面临着数据无法存储、无法计算的状态的形容词。</p>
<p>那么大数据可以做什么呢？</p>
<p>在海量的各种各样类型的价值密度低的数据中，我们要进行的是:数据采集，数据存储，数据清洗，数据分析，数据可视化。</p>
<p>简单一句话，就是大数据让数据产生各种”价值”。可以说，大数据的核心作用就是”数据价值化”，这个过程就是大数据要做的主要事情。那么就可以概括成：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- 记录已经发生的一切</span><br><span class="line">- 描述正在发生的一切</span><br><span class="line">- 预测将要发生的一切</span><br></pre></td></tr></table></figure>

<p>大数据技术的战略意义不在于掌握庞大的数据信息，而在于<strong>对这些含有意义的数据进行专业化处理</strong>。 </p>
<p>现在已经应用”大数据”的案例有：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- 预测犯罪</span><br><span class="line">- 预测流感的爆发</span><br><span class="line">- 预测选举</span><br><span class="line">- 根据手机定位和交通数据，规划城市</span><br><span class="line">- 根据库存和需求，实时调价</span><br><span class="line">- 推动医疗信息化发展，远程医疗</span><br></pre></td></tr></table></figure>



<h3 id="1-4-大数据的发展前景"><a href="#1-4-大数据的发展前景" class="headerlink" title="1.4 大数据的发展前景"></a>1.4 大数据的发展前景</h3><p>大数据技术目前正处在落地应用的初期，从大数据自身发展和行业发展的趋势来看，大数据未来的前景还是不错的，具体原因有以下几点：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">- 大数据本身的价值体现，</span><br><span class="line">	本身的数据价值化就会开辟出很大的市场空间。目前在互联网领域，大数据技术已经得到了较为广泛的应用。 大数据造就了新兴行业</span><br><span class="line">	</span><br><span class="line">- 大数据推动了科技领域的发展</span><br><span class="line">	不仅体现在互联网领域，还体现在金融、教育、医疗等诸多领域，尤其是现在的人工智能。</span><br><span class="line">	</span><br><span class="line">- 大数据产业链的形成</span><br><span class="line">	经过近些年的发展，大数据已经初步形成了一个较为完整的产业链，包括数据采集、整理、传输、存储、分析、呈现和应用，众多企业开始参与到大数据产业链中，并形成了一定的产业规模，相信随着大数据的不断发展，相</span><br><span class="line">	关产业规模会进一步扩大。</span><br><span class="line">	</span><br><span class="line">- 国家大力扶持大数据行业的发展</span><br></pre></td></tr></table></figure>



<h2 id="第二章-Hadoop概述"><a href="#第二章-Hadoop概述" class="headerlink" title="第二章 Hadoop概述"></a>第二章 Hadoop概述</h2><h3 id="2-1-为什么要用hadoop"><a href="#2-1-为什么要用hadoop" class="headerlink" title="2.1 为什么要用hadoop"></a>2.1 为什么要用hadoop</h3><p>现在的我们，生活在数据大爆炸的年代。2020年，全球的数据总量达到44ZB，经过单位换算后，至少在440亿TB以上，也就是说，全球每人一块1TB的硬盘都存储不下。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">扩展: 数据大小单位，从小到大分别是: byte、kb、mb、Gb、Tb、PB、EB、ZB、DB、NB...</span><br><span class="line">单位之间的转换都是满足1024</span><br></pre></td></tr></table></figure>

<p> 一些数据集的大小更远远超过了1TB，也就是说，数据的存储是一个要解决的问题。同时，硬盘技术也面临一个技术瓶颈，就是硬盘的传输速度(读数据的速度)的提升远远低于硬盘容量的提升。我们看下面这个表格: </p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401012832988.png" alt="image-20210401012832988"></p>
<p> 可以看到，容量提升了将近1000倍，而传输速度才提升了20倍，读完一个硬盘的所需要的时间相对来说，更长更久了(已经违反了数据价值的即时性)。读数据都花了这么长时间，更不用说写数据了。 </p>
<p>对于如何提高读取数据的效率，我们已经想到解决的方法了，那就是将一个数据集存储到多个硬盘里，然后并行读取。比如1T的数据，我们平均100份存储到100个1TB硬盘上，同时读取，那么读取完整个数据集的时间用不上两分钟。至于硬盘剩下的99%的容量，我们可以用来存储其他的数据集，这样就不会产生浪费。解决读取效率问题的同时，我们也解决了大数据的存储问题。</p>
<p> 但是，我们同时对多个硬盘进行读/写操作时，又有了新的问题需要解决：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1、硬件故障问题。一旦使用多个硬件，相对来说，个别硬件产生故障的几率就高，为了避免数据丢失，最常见的做法就是复制(replication):文件系统保存数据的多个复本，一旦发生故障，就可以使用另外的复本。</span><br><span class="line"></span><br><span class="line">2、读取数据的正确性问题。大数据时代的一个分析任务，就需要结合大部分数据来共同完成分析，因此从一个硬盘上读取的数据要与从其他99个硬盘上读取的数据结合起来使用。那么，在读取过程中，如何保证数据的正确性，就是一个很大的挑战。</span><br></pre></td></tr></table></figure>

<p> 针对于上述几个问题，Hadoop为我们提供了一个可靠的且可扩展的存储和分析平台，此外，由于Hadoop运行在商用硬件上且是开源的，因此Hadoop的使用成本是比较低了，在用户的承受范围内。</p>
<h3 id="2-2-Hadoop的简要介绍"><a href="#2-2-Hadoop的简要介绍" class="headerlink" title="2.2 Hadoop的简要介绍"></a>2.2 Hadoop的简要介绍</h3><p>Hadoop是Apache基金会旗下一个开源的分布式存储和分析计算平台，使用java语言开发，具有很好的跨平台性，可以运行在商用(廉价)硬件上，用户无需了解分布式底层细节，就可以开发分布式程序，充分使用集群的高速计算和存储</p>
<pre><code>Apache lucene是一个应用广泛的文本搜索系统库。该项目的创始人道格·卡丁在2002年带领团队开发该项目中的子项目Apache Nutch，想要从头打造一个网络搜索引擎系统，在开发的过程中，发现了两个问题，一个是硬件的高额资金投入，另一个是存储问题。

2003年和2004年Google先后发表的《GFS》和《MapReduce》论文，给这个团队提供了灵感，并进行了实现，于是NDFS(Nutch分布式文件系统)和MapReduce相继问世。2006年2月份，开发人员将NDFS和MapReduce移出Nutch形成一个独立的子项目，命名为Hadoop(该名字据Doug Cutting所说，是借用了他的孩子给毛绒玩具取得名字)。
</code></pre>
<h3 id="2-3-谷歌的三篇论文"><a href="#2-3-谷歌的三篇论文" class="headerlink" title="2.3 谷歌的三篇论文"></a>2.3 谷歌的三篇论文</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- 2003年发表的《GFS》</span><br><span class="line">	基于硬盘不够大、数据存储单份的安全隐患问题，提出的分布式文件系统用于存储的理论思想。</span><br><span class="line">	· 解决了如何存储大数据集的问题</span><br><span class="line"></span><br><span class="line">- 2004年发表的《MapReduce》</span><br><span class="line"> 	基于分布式文件系统的计算分析的编程框架模型。移动计算而非移动数据，分而治之。</span><br><span class="line">	· 解决了如何快速分析大数据集的问题</span><br><span class="line"></span><br><span class="line">- 2006年发表的《BigTable》</span><br><span class="line">	针对于传统型关系数据库不适合存储非结构化数据的缺点，提出了另一种适合存储大数据集的解决方案</span><br></pre></td></tr></table></figure>



<h3 id="2-4-Hadoop的发展历史"><a href="#2-4-Hadoop的发展历史" class="headerlink" title="2.4 Hadoop的发展历史"></a>2.4 Hadoop的发展历史</h3><p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401013100075.png" alt="image-20210401013100075"></p>
<h3 id="2-5-Hadoop的组成部分"><a href="#2-5-Hadoop的组成部分" class="headerlink" title="2.5 Hadoop的组成部分"></a>2.5 Hadoop的组成部分</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hadoop2.0以后的四个模块：</span><br><span class="line">    - Hadoop Common:Hadoop模块的通用组件</span><br><span class="line">    - Hadoop Distributed File System：分布式文件系统</span><br><span class="line">    - Hadoop YARN：作业调度和资源管理框架</span><br><span class="line">    - Hadoop MapReduce：基于YARN的大型数据集并行计算处理框架</span><br><span class="line"></span><br><span class="line">hadoop3.0新扩展的两个模块：</span><br><span class="line">    - Hadoop Ozone:Hadoop的对象存储机制 </span><br><span class="line">    - Hadoop Submarine:Hadoop的机器学习引擎</span><br></pre></td></tr></table></figure>



<h3 id="2-6-Hadoop的生态系统"><a href="#2-6-Hadoop的生态系统" class="headerlink" title="2.6. Hadoop的生态系统"></a>2.6. Hadoop的生态系统</h3><p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401013159672.png" alt="image-20210401013159672"></p>
<h2 id="第三章-Hadoop集群安装"><a href="#第三章-Hadoop集群安装" class="headerlink" title="第三章 Hadoop集群安装"></a>第三章 Hadoop集群安装</h2><h3 id="3-1-集群规划"><a href="#3-1-集群规划" class="headerlink" title="3.1 集群规划"></a>3.1 集群规划</h3><table>
<thead>
<tr>
<th>集群规划</th>
<th>规划</th>
</tr>
</thead>
<tbody><tr>
<td>操作系统</td>
<td>Mac、Windows</td>
</tr>
<tr>
<td>虚拟软件</td>
<td>Parallels Desktop(Mac)、VMWare(Windows)</td>
</tr>
<tr>
<td>虚拟机</td>
<td>主机名: qianfeng01, IP地址: 192.168.10.101<br />主机名: qianfeng02, IP地址: 192.168.10.102<br />主机名: qianfeng03, IP地址: 192.168.10.103</td>
</tr>
<tr>
<td>软件包上传路径</td>
<td>/root/softwares</td>
</tr>
<tr>
<td>软件包安装路径</td>
<td>/usr/local</td>
</tr>
<tr>
<td>JDK</td>
<td>Jdk-8u221-linux-x64.tar.gz</td>
</tr>
<tr>
<td>Hadoop</td>
<td>hadoop-2.7.6.tar.gz</td>
</tr>
<tr>
<td>用户</td>
<td>root</td>
</tr>
</tbody></table>
<h3 id="3-2-安装JDK"><a href="#3-2-安装JDK" class="headerlink" title="3.2 安装JDK"></a>3.2 安装JDK</h3><h4 id="3-2-1-检查一下是否已经安装过或者系统内置JDK-如果有内置的，将其卸载"><a href="#3-2-1-检查一下是否已经安装过或者系统内置JDK-如果有内置的，将其卸载" class="headerlink" title="3.2.1 检查一下是否已经安装过或者系统内置JDK,如果有内置的，将其卸载"></a>3.2.1 检查一下是否已经安装过或者系统内置JDK,如果有内置的，将其卸载</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]# rpm -qa | grep jdk     				# 如果有,请卸载</span><br><span class="line">[root@qianfeng01 ~]# rpm -e xxxxxxxx --nodeps      	# 将查询到的内置jdk强制卸载</span><br></pre></td></tr></table></figure>

<h4 id="3-2-2-上传jdk1-8"><a href="#3-2-2-上传jdk1-8" class="headerlink" title="3.2.2 上传jdk1.8"></a>3.2.2 上传jdk1.8</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">将jdk-8u221-linux-x64.tar.gz上传到/root目录中</span><br></pre></td></tr></table></figure>

<h4 id="3-2-3-解压jdk到-usr-local-下"><a href="#3-2-3-解压jdk到-usr-local-下" class="headerlink" title="3.2.3 解压jdk到/usr/local/下"></a>3.2.3 解压jdk到/usr/local/下</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]# tar -zxvf jdk-8u221-linux-x64.tar.gz -C /usr/local</span><br></pre></td></tr></table></figure>

<h4 id="3-2-4-更名jdk"><a href="#3-2-4-更名jdk" class="headerlink" title="3.2.4 更名jdk"></a>3.2.4 更名jdk</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]# cd /usr/local</span><br><span class="line">[root@qianfeng01 local]# mv jdk1.8.0_221/  jdk</span><br></pre></td></tr></table></figure>

<h4 id="3-2-5-配置Jdk的环境变量：-etc-profile"><a href="#3-2-5-配置Jdk的环境变量：-etc-profile" class="headerlink" title="3.2.5 配置Jdk的环境变量：/etc/profile"></a>3.2.5 配置Jdk的环境变量：/etc/profile</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 local]# vi /etc/profile</span><br><span class="line">.........省略...........</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">jdk environment</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk</span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH</span><br></pre></td></tr></table></figure>

<h4 id="3-2-6-使当前窗口生效"><a href="#3-2-6-使当前窗口生效" class="headerlink" title="3.2.6 使当前窗口生效"></a>3.2.6 使当前窗口生效</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 local]# source /etc/profile</span><br></pre></td></tr></table></figure>

<h4 id="3-2-7-验证jdk环境"><a href="#3-2-7-验证jdk环境" class="headerlink" title="3.2.7 验证jdk环境"></a>3.2.7 验证jdk环境</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 local]# java -version</span><br><span class="line">[root@qianfeng01 local]# javac	</span><br></pre></td></tr></table></figure>



<h3 id="3-3-完全分布式环境需求及安装"><a href="#3-3-完全分布式环境需求及安装" class="headerlink" title="3.3. 完全分布式环境需求及安装"></a>3.3. 完全分布式环境需求及安装</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 三台机器的防火墙必须是关闭的</span><br><span class="line">2. 确保三台机器的网络配置通常（NAT模式、静态IP、主机名的配置）</span><br><span class="line">3. 确保/etc/hosts文件配置了IP和hosts的映射关系</span><br><span class="line">4. 确保配置了三台机器的免密登录认证</span><br><span class="line">5. 确保所有的机器时间同步</span><br><span class="line">6. JDK和Hadoop的环境变量配置</span><br></pre></td></tr></table></figure>

<h4 id="3-3-1-关闭防火墙"><a href="#3-3-1-关闭防火墙" class="headerlink" title="3.3.1 关闭防火墙"></a>3.3.1 关闭防火墙</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]# systemctl stop firewalld</span><br><span class="line">[root@qianfeng01 ~]# systemctl disable firewalld</span><br><span class="line">[root@qianfeng01 ~]# systemctl stop NetworkManager</span><br><span class="line">[root@qianfeng01 ~]# systemctl disable NetworkManager</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">最好也把selinux关闭掉，这是linux系统的一个安全机制，进入文件中将SELINUX设置为disabled</span></span><br><span class="line">[root@qianfeng01 ~]# vi /etc/selinux/config</span><br><span class="line">.........</span><br><span class="line">SELINUX=disabled			</span><br><span class="line">.........</span><br></pre></td></tr></table></figure>

<h4 id="3-3-2-静态IP和主机名配置"><a href="#3-3-2-静态IP和主机名配置" class="headerlink" title="3.3.2 静态IP和主机名配置"></a>3.3.2 静态IP和主机名配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">--1. 配置静态IP（确保NAT模式）</span><br><span class="line">[root@qianfeng01 ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">............</span><br><span class="line">BOOTPROTO=static					# 将dhcp改为static</span><br><span class="line">............</span><br><span class="line">ONBOOT=yes								# 将no改为yes</span><br><span class="line">IPADDR=192.168.10.101			# 添加IPADDR属性和ip地址</span><br><span class="line">PREFIX=24									# 添加NETMASK=255.255.255.0或者PREFIX=24	</span><br><span class="line">GATEWAY=192.168.10.2			# 添加网关GATEWAY</span><br><span class="line">DNS1=114.114.114.114      # 添加DNS1和备份DNS</span><br><span class="line">DNS2=8.8.8.8</span><br><span class="line"></span><br><span class="line">--2. 重启网络服务</span><br><span class="line">[root@qianfeng01 ~]# systemctl restart network</span><br><span class="line">或者</span><br><span class="line">[root@qianfeng01 ~]# service network restart</span><br><span class="line"></span><br><span class="line">--3. 修改主机名(如果修改过，请略过这一步)</span><br><span class="line">[root@localhost ~]# hostnamectl set-hostname qianfeng01</span><br><span class="line">或者</span><br><span class="line">[root@localhost ~]# vi /etc/hostname</span><br><span class="line">qianfeng01</span><br></pre></td></tr></table></figure>

<h4 id="3-3-3-配置-etc-hosts文件"><a href="#3-3-3-配置-etc-hosts文件" class="headerlink" title="3.3.3 配置/etc/hosts文件"></a>3.3.3 配置/etc/hosts文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]#  vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.10.101 qianfeng01  #添加本机的静态IP和本机的主机名之间的映射关系 </span><br><span class="line">192.168.10.102 qianfeng02</span><br><span class="line">192.168.10.103 qianfeng03</span><br></pre></td></tr></table></figure>

<h4 id="3-3-4-免密登录认证"><a href="#3-3-4-免密登录认证" class="headerlink" title="3.3.4 免密登录认证"></a>3.3.4 免密登录认证</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">-1. 使用rsa加密技术，生成公钥和私钥。一路回车即可</span><br><span class="line">[root@qianfeng01 ~]# cd ~</span><br><span class="line">[root@qianfeng01 ~]# ssh-keygen -t rsa	</span><br><span class="line"></span><br><span class="line">-2. 进入~/.ssh目录下，使用ssh-copy-id命令</span><br><span class="line">[root@qianfeng01 ~]# cd ~/.ssh			</span><br><span class="line">[root@qianfeng01 .ssh]# ssh-copy-id  root@qianfeng01</span><br><span class="line"></span><br><span class="line">-3. 进行验证	</span><br><span class="line">[hadoop@qianfeng01 .ssh]# ssh qianfeng01</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">下面的第一次执行时输入<span class="built_in">yes</span>后，不提示输入密码就对了</span></span><br><span class="line">[hadoop@qianfeng01 .ssh]# ssh localhost</span><br><span class="line">[hadoop@qianfeng01 .ssh]# ssh 0.0.0.0</span><br><span class="line"></span><br><span class="line">注意：三台机器提前安装好的情况下，需要同步公钥文件。如果使用克隆技术。那么使用同一套密钥对就方便多了。</span><br></pre></td></tr></table></figure>

<h4 id="3-3-5-时间同步"><a href="#3-3-5-时间同步" class="headerlink" title="3.3.5 时间同步"></a>3.3.5 时间同步</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1 选择集群中的某一台机器作为时间服务器，例如qianfeng01</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2 保证这台服务器安装了ntp.x86_64。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3 保证ntpd 服务运行......</span></span><br><span class="line">[root@qianfeng01 ~]# sudo service ntpd start</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">	开机自启动:</span></span><br><span class="line">[root@qianfeng01 ~]# chkconfig ntpd on</span><br><span class="line"><span class="meta prompt_">	</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">4 配置相应文件：</span></span><br><span class="line">[root@qianfeng01 ~]# vi /etc/ntp.conf</span><br><span class="line"><span class="meta prompt_">	</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">Hosts on <span class="built_in">local</span> network are less restricted.</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span></span><br><span class="line"><span class="meta prompt_">  # </span><span class="language-bash">添加集群中的网络段位</span></span><br><span class="line">	restrict 192.168.10.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">Use public servers from the pool.ntp.org project.</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">Please consider joining the pool (http://www.pool.ntp.org/join.html).</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">server 0.centos.pool.ntp.org iburst    注释掉</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">server 1.centos.pool.ntp.org iburst	   注释掉</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">server 2.centos.pool.ntp.org iburst    注释掉</span></span><br><span class="line"><span class="meta prompt_">	# </span><span class="language-bash">server 3.centos.pool.ntp.org iburst    注释掉</span></span><br><span class="line">	server 127.127.1.0     -master作为服务器</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">5 其他机器要保证安装ntpdate.x86_64</span></span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">6 其他机器要使用root定义定时器</span></span><br><span class="line">*/1 * * * * /usr/sbin/ntpdate -u qianfeng01 </span><br></pre></td></tr></table></figure>

<h4 id="3-3-6-Hadoop安装与环境变量配置"><a href="#3-3-6-Hadoop安装与环境变量配置" class="headerlink" title="3.3.6 Hadoop安装与环境变量配置"></a>3.3.6 Hadoop安装与环境变量配置</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">1. 上传和解压两个软件包</span></span><br><span class="line">[root@qianfeng01 ~]# tar -zxvf jdk-8u221-linux-x64.tar.gz -C /usr/local/</span><br><span class="line">[root@qianfeng01 ~]# tar -zxvf hadoop-2.7.6.tar.gz -C /usr/local/</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">2. 进入<span class="built_in">local</span>里，给两个软件更名</span></span><br><span class="line">[root@qianfeng01 ~]# cd /usr/local/</span><br><span class="line">[root@qianfeng01 local]# mv 1.8.0_221/  jdk</span><br><span class="line">[root@qianfeng01 local]# mv hadoop-2.7.6/ hadoop</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">3. 配置环境变量</span></span><br><span class="line">[hadoop@qianfeng01 local]# vi /etc/profile</span><br><span class="line"></span><br><span class="line">.....省略...........</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">java environment</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk</span><br><span class="line">export PATH=$JAVA_HOME/bin:$JAVA_HOME/jre/bin:$PATH</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">hadoop environment</span></span><br><span class="line">export HADOOP_HOME=/usr/local/hadoop</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH</span><br></pre></td></tr></table></figure>



<h3 id="3-4-Hadoop的配置文件"><a href="#3-4-Hadoop的配置文件" class="headerlink" title="3.4. Hadoop的配置文件"></a>3.4. Hadoop的配置文件</h3><h4 id="3-4-1-概述"><a href="#3-4-1-概述" class="headerlink" title="3.4.1. 概述"></a>3.4.1. 概述</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">我们需要通过配置若干配置文件，来实现Hadoop集群的配置信息。需要配置的文件有:</span><br><span class="line">hadoop-env.sh</span><br><span class="line">yarn-env.sh</span><br><span class="line">core-site.xml</span><br><span class="line">hdfs-site.xml</span><br><span class="line">mapred-site.xml</span><br><span class="line">yarn-site.xml</span><br><span class="line"></span><br><span class="line">在Hadoop安装完成后，会在$HADOOP_HOME/share路径下，有若干个*-default.xml文件，这些文件中记录了默认的配置信息。同时，在代码中，我们也可以设置Hadoop的配置信息。</span><br><span class="line">这些位置配置的Hadoop，优先级为: 代码设置 &gt; *-site.xml &gt; *-default.xml</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">集群规划:</span><br><span class="line">+--------------+---------------------+</span><br><span class="line">|     Node     | Applications        |</span><br><span class="line">+--------------+---------------------+</span><br><span class="line">|  qianfeng01  | NameNode            |</span><br><span class="line">|              | DataNode            |</span><br><span class="line">|              | ResourceManager     |</span><br><span class="line">|              | NodeManagere        |</span><br><span class="line">+--------------+---------------------+</span><br><span class="line">|  qianfeng02  | SecondaryNameNode   |</span><br><span class="line">|              | DataNode            |</span><br><span class="line">|              | NodeManager         |</span><br><span class="line">+--------------+---------------------+</span><br><span class="line">|  qianfeng03  | DataNode            |</span><br><span class="line">|              | NodeManager         |</span><br><span class="line">+--------------+---------------------+</span><br></pre></td></tr></table></figure>




<h4 id="3-4-2-core-site-xml"><a href="#3-4-2-core-site-xml" class="headerlink" title="3.4.2. core-site.xml"></a>3.4.2. core-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]# cd $HADOOP_HOME/etc/hadoop/</span><br><span class="line">[root@qianfeng01 hadoop]# vi core-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- hdfs的地址名称：schame,ip,port--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 在Hadoop1.x的版本中，默认使用的端口是9000。在Hadoop2.x的版本中，默认使用端口是8020 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://qianfeng01:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- hdfs的基础路径，被其他属性所依赖的一个基础路径 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-4-3-hdfs-site-xml"><a href="#3-4-3-hdfs-site-xml" class="headerlink" title="3.4.3. hdfs-site.xml"></a>3.4.3. hdfs-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 hadoop]# vi hdfs-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- namenode守护进程管理的元数据文件fsimage存储的位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 确定DFS数据节点应该将其块存储在本地文件系统的何处--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 块的副本数--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 块的大小(128M),下面的单位是字节--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- secondarynamenode守护进程的http地址：主机名和端口号。参考守护进程布局--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng02:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  	<span class="comment">&lt;!-- namenode守护进程的http地址：主机名和端口号。参考守护进程布局--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  	  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  	  <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span>  </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-4-4-mapred-site-xml"><a href="#3-4-4-mapred-site-xml" class="headerlink" title="3.4.4. mapred-site.xml"></a>3.4.4. mapred-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 hadoop]# cp mapred-site.xml.template  mapred-site.xml</span><br><span class="line">[root@qianfeng01 hadoop]# vi mapred-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定mapreduce使用yarn资源管理器--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置作业历史服务器的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 配置作业历史服务器的http地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-4-5-yarn-site-xml"><a href="#3-4-5-yarn-site-xml" class="headerlink" title="3.4.5 yarn-site.xml"></a>3.4.5 yarn-site.xml</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 hadoop]# vi yarn-site.xml</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定yarn的shuffle技术--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">       <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定resourcemanager的主机名--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="comment">&lt;!--下面的可选--&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定shuffle对应的类 --&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!--配置resourcemanager的内部通讯地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!--配置resourcemanager的scheduler的内部通讯地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!--配置resoucemanager的资源调度的内部通讯地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!--配置resourcemanager的管理员的内部通讯地址--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">	<span class="comment">&lt;!--配置resourcemanager的web ui 的监控页面--&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h4 id="3-4-6-hadoop-env-sh"><a href="#3-4-6-hadoop-env-sh" class="headerlink" title="3.4.6 hadoop-env.sh"></a>3.4.6 hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 hadoop]# vi hadoop-env.sh</span><br><span class="line">.........</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">The java implementation to use.</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk</span><br><span class="line">.........</span><br></pre></td></tr></table></figure>



<h4 id="3-4-7-yarn-env-sh"><a href="#3-4-7-yarn-env-sh" class="headerlink" title="3.4.7 yarn-env.sh"></a>3.4.7 yarn-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 hadoop]# vi yarn-env.sh</span><br><span class="line">.........</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">some Java parameters</span></span><br><span class="line">export JAVA_HOME=/usr/local/jdk</span><br><span class="line">if [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then</span><br><span class="line"><span class="meta prompt_">  #</span><span class="language-bash"><span class="built_in">echo</span> <span class="string">&quot;run java in <span class="variable">$JAVA_HOME</span>&quot;</span></span></span><br><span class="line">  JAVA_HOME=$JAVA_HOME</span><br><span class="line">fi</span><br><span class="line">.........</span><br></pre></td></tr></table></figure>



<h4 id="3-4-8-slaves"><a href="#3-4-8-slaves" class="headerlink" title="3.4.8 slaves"></a>3.4.8 slaves</h4><p>此文件用于指定datanode守护进程所在的机器节点主机名 </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 hadoop]# vi slaves</span><br><span class="line">qianfeng01</span><br><span class="line">qianfeng02</span><br><span class="line">qianfeng03</span><br></pre></td></tr></table></figure>



<h4 id="3-4-9-分发到另外两台节点"><a href="#3-4-9-分发到另外两台节点" class="headerlink" title="3.4.9 分发到另外两台节点"></a>3.4.9 分发到另外两台节点</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同步Hadoop到另外两台节点</span></span><br><span class="line">[root@qianfeng01 ~]# cd /usr/local</span><br><span class="line">[root@qianfeng02 local]# scp -r hadoop qianfeng02:$PWD</span><br><span class="line">[root@qianfeng02 local]# scp -r hadoop qianfeng03:$PWD</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">同步profile到另外两台节点</span></span><br><span class="line">[root@qianfeng01 ~]# scp /etc/profile qianfeng02:/etc</span><br><span class="line">[root@qianfeng01 ~]# scp /etc/profile qianfeng03:/etc</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查slave节点上的jdk是否已安装</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">检查是否同步了/etc/hosts文件</span></span><br></pre></td></tr></table></figure>



<h3 id="3-5-格式化与启动"><a href="#3-5-格式化与启动" class="headerlink" title="3.5 格式化与启动"></a>3.5 格式化与启动</h3><h4 id="3-5-1-格式化集群"><a href="#3-5-1-格式化集群" class="headerlink" title="3.5.1 格式化集群"></a>3.5.1 格式化集群</h4><p><strong>1）</strong>在qianfeng01机器上运行命令</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]<span class="comment"># hdfs namenode -format</span></span><br></pre></td></tr></table></figure>

<p><strong>2）</strong>格式化的相关信息解读</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--1. 生成一个集群唯一标识符:clusterid</span><br><span class="line">--2. 生成一个块池唯一标识符:blockPoolId</span><br><span class="line">--3. 生成namenode进程管理内容(fsimage)的存储路径：</span><br><span class="line">	默认配置文件属性hadoop.tmp.dir指定的路径下生成dfs/name目录</span><br><span class="line">--4. 生成镜像文件fsimage，记录分布式文件系统根路径的元数据</span><br><span class="line"></span><br><span class="line">--5. 其他信息都可以查看一下，比如块的副本数，集群的fsOwner等。</span><br></pre></td></tr></table></figure>

<p>参考图片：</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104441770.png" alt="image-20210401104441770"></p>
<p><strong>3)</strong> 目录里的内容查看</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104504725.png" alt="image-20210401104504725"></p>
<h4 id="3-5-2-启动集群"><a href="#3-5-2-启动集群" class="headerlink" title="3.5.2 启动集群"></a>3.5.2 启动集群</h4><p><strong>1)</strong> 启动脚本和关闭脚本介绍</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1. 启动脚本</span><br><span class="line">	-- start-dfs.sh			:用于启动hdfs集群的脚本</span><br><span class="line">	-- start-yarn.sh		:用于启动yarn守护进程</span><br><span class="line">	-- start-all.sh			:用于启动hdfs和yarn</span><br><span class="line">2. 关闭脚本</span><br><span class="line">	-- stop-dfs.sh			:用于关闭hdfs集群的脚本</span><br><span class="line">	-- stop-yarn.sh			:用于关闭yarn守护进程</span><br><span class="line">	-- stop-all.sh			:用于关闭hdfs和yarn</span><br><span class="line">3. 单个守护进程脚本</span><br><span class="line">	-- hadoop-daemons.sh	:用于单独启动或关闭hdfs的某一个守护进程的脚本</span><br><span class="line">	-- hadoop-daemon.sh		:用于单独启动或关闭hdfs的某一个守护进程的脚本</span><br><span class="line">	reg:</span><br><span class="line">		hadoop-daemon.sh [start|stop] [namenode|datanode|secondarynamenode]</span><br><span class="line">	</span><br><span class="line">	-- yarn-daemons.sh	:用于单独启动或关闭hdfs的某一个守护进程的脚本</span><br><span class="line">	-- yarn-daemon.sh		:用于单独启动或关闭hdfs的某一个守护进程的脚本</span><br><span class="line">	reg:</span><br><span class="line">		yarn-daemon.sh [start|stop] [resourcemanager|nodemanager]</span><br></pre></td></tr></table></figure>

<p>**2) ** 启动HDFS</p>
<p>使用start-dfs.sh，启动 hdfs。参考图片</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104625494.png" alt="image-20210401104625494"></p>
<p>启动过程解析：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">- 启动集群中的各个机器节点上的分布式文件系统的守护进程</span><br><span class="line">  一个namenode和resourcemanager以及secondarynamenode</span><br><span class="line">  多个datanode和nodemanager</span><br><span class="line">- 在namenode守护进程管理内容的目录下生成edit日志文件</span><br><span class="line">- 在每个datanode所在节点下生成$&#123;hadoop.tmp.dir&#125;/dfs/data目录,参考下图：</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104650560.png" alt="image-20210401104650560"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">注意！</span><br><span class="line">如果哪台机器的相关守护进程没有开启，那么，就查看哪台机器上的守护进程对应的日志log文件,注意，启动脚本运行时提醒的日志后缀是*.out，而我们查看的是*.log文件。此文件的位置：$&#123;HADOOP_HOME&#125;/logs/里</span><br></pre></td></tr></table></figure>

<p><strong>3)</strong>  jps查看进程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">--1. 在qianfeng01上运行jps指令，会有如下进程</span><br><span class="line">	namenode</span><br><span class="line">	datanode</span><br><span class="line">--2. 在qianfeng02上运行jps指令，会有如下进程</span><br><span class="line">	secondarynamenode</span><br><span class="line">	datanode</span><br><span class="line">--3. 在qianfeng03上运行jps指令，会有如下进程</span><br><span class="line">	datanode   </span><br></pre></td></tr></table></figure>

<p>**4) **启动yarn</p>
<p>使用start-yarn.sh脚本，参考图片</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401104811173.png" alt="image-20210401104811173"></p>
<p>jps查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">--1. 在qianfeng01上运行jps指令，会多出有如下进程</span><br><span class="line">	resoucemanager</span><br><span class="line">	nodemanager</span><br><span class="line">--2. 在qianfeng02上运行jps指令，会多出有如下进程</span><br><span class="line">	nodemanager</span><br><span class="line">--3. 在qianfeng03上运行jps指令，会多出有如下进程</span><br><span class="line">	nodemanager </span><br></pre></td></tr></table></figure>



<p><strong>5)</strong> webui查看</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HDFS: http://192.168.10.101:50070</span><br><span class="line">YARN: http://192.168.10.101:8088</span><br></pre></td></tr></table></figure>



<h2 id="第四章-HDFS的Shell命令"><a href="#第四章-HDFS的Shell命令" class="headerlink" title="第四章 HDFS的Shell命令"></a>第四章 HDFS的Shell命令</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">HDFS其实就是一个分布式的文件系统，我们可以使用一些命令来操作这个分布式文件系统上的文件。</span><br><span class="line">- 访问HDFS的命令:</span><br><span class="line">  hadoop dfs --- 已过时</span><br><span class="line">  hdfs dfs</span><br><span class="line">  </span><br><span class="line">- 小技巧</span><br><span class="line">  1. 在命令行中输入hdfs，回车后，就会提示hdfs后可以使用哪些命令，其中有一个是dfs。</span><br><span class="line">  2. 在命令行中输入hdfs dfs，回车后，就会提示dfs后可以添加的一些常用shell命令。</span><br><span class="line">  </span><br><span class="line">- 注意事项</span><br><span class="line">  分布式文件系统的路径在命令行中，要从/开始写，即绝对路径。</span><br></pre></td></tr></table></figure>

<h3 id="4-1-创建目录"><a href="#4-1-创建目录" class="headerlink" title="4.1 创建目录"></a>4.1 创建目录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[-mkdir [-p] &lt;path&gt; ...]	#在分布式文件系统上创建目录  -p,多层级创建</span><br><span class="line"></span><br><span class="line">调用格式: hdfs dfs -mkdir (-p)  /目录</span><br><span class="line">例如: </span><br><span class="line">    - hdfs dfs -mkdir /data</span><br><span class="line">    - hdfs dfs -mkdir -p /data/a/b/c</span><br></pre></td></tr></table></figure>



<h3 id="4-2-上传指令"><a href="#4-2-上传指令" class="headerlink" title="4.2 上传指令"></a>4.2 上传指令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]   #将本地文件系统的文件上传到分布式文件系统</span><br><span class="line"></span><br><span class="line">调用格式:hdfs dfs -put /本地文件  /分布式文件系统路径</span><br><span class="line">注意: 直接写/是省略了文件系统的名称hdfs://ip:port。</span><br><span class="line">例如:</span><br><span class="line">    - hdfs dfs -put /root/a.txt /data/</span><br><span class="line">    - hdfs dfs -put /root/logs/* /data/</span><br><span class="line"></span><br><span class="line">其他指令:</span><br><span class="line">    [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;]		#将本地文件系统的文件上传到分布式文件系统</span><br><span class="line">    [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;]</span><br></pre></td></tr></table></figure>



<h3 id="4-3-创建空文件"><a href="#4-3-创建空文件" class="headerlink" title="4.3 创建空文件"></a>4.3 创建空文件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs [generic options] -touchz &lt;path&gt; ...   </span><br><span class="line">调用格式:hdfs dfs touchz  /hadooptest.txt</span><br></pre></td></tr></table></figure>



<h3 id="4-4-向分布式文件系统中的文件里追加内容"><a href="#4-4-向分布式文件系统中的文件里追加内容" class="headerlink" title="4.4 向分布式文件系统中的文件里追加内容"></a>4.4 向分布式文件系统中的文件里追加内容</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;]</span><br><span class="line">调用格式:hdfs dfs -appendToFile  本地文件     hdfs上的文件</span><br><span class="line">注意:不支持在中间随意增删改操作</span><br></pre></td></tr></table></figure>



<h3 id="4-5-查看指令"><a href="#4-5-查看指令" class="headerlink" title="4.5 查看指令"></a>4.5 查看指令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[-ls [-d] [-h] [-R] [&lt;path&gt; ...]]		#查看分布式文件系统的目录里内容</span><br><span class="line">调用格式:hdfs dfs -ls /</span><br><span class="line"></span><br><span class="line">[-cat [-ignoreCrc] &lt;src&gt; ...]	    	#查看分布式文件系统的文件内容	</span><br><span class="line">调用格式:hdfs dfs -cat /xxx.txt</span><br><span class="line"></span><br><span class="line">[-tail [-f] &lt;file&gt;]						#查看分布式文件系统的文件内容	</span><br><span class="line">调用格式:hdfs dfs -tail /xxx.txt</span><br><span class="line">注意:默认最多查看1000行</span><br></pre></td></tr></table></figure>



<h3 id="4-6-下载指令"><a href="#4-6-下载指令" class="headerlink" title="4.6 下载指令"></a>4.6 下载指令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]</span><br><span class="line">注意:本地路径的文件夹可以不存在</span><br><span class="line"></span><br><span class="line">[-moveToLocal &lt;src&gt; &lt;localdst&gt;]</span><br><span class="line">注意:从hdfs的某个路径将数据剪切到本地,已经被遗弃了</span><br><span class="line"></span><br><span class="line">[-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;]	</span><br><span class="line">调用格式:同copyToLocal</span><br></pre></td></tr></table></figure>



<h3 id="4-7-合并下载"><a href="#4-7-合并下载" class="headerlink" title="4.7 合并下载"></a>4.7 合并下载</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs [generic options] -getmerge [-nl] &lt;src&gt; &lt;localdst&gt;</span><br><span class="line">调用格式:hdfs dfs -getmerge  hdfs上面的路径   本地的路径    </span><br><span class="line">实例:hdfs dfs -getmerge /hadoopdata/*.xml /root/test.test</span><br></pre></td></tr></table></figure>



<h3 id="4-8-移动hdfs中的文件（更名）"><a href="#4-8-移动hdfs中的文件（更名）" class="headerlink" title="4.8  移动hdfs中的文件（更名）"></a>4.8  移动hdfs中的文件（更名）</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfds [generic options] -mv &lt;src&gt; ... &lt;dst&gt;   </span><br><span class="line">调用格式:hdfs dfs -mv /hdfs的路径1  /hdfs的另一个路径2    </span><br><span class="line">实例:hfds dfs -mv /aaa   /bbb  这里是将aaa整体移动到bbb中</span><br></pre></td></tr></table></figure>



<h3 id="4-9-复制hdfs中的文件到hdfs的另一个目录"><a href="#4-9-复制hdfs中的文件到hdfs的另一个目录" class="headerlink" title="4.9 复制hdfs中的文件到hdfs的另一个目录"></a>4.9 复制hdfs中的文件到hdfs的另一个目录</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs [generic options] -cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;</span><br><span class="line">调用格式:hdfs dfs -cp /hdfs路径_1  /hdfs路径_2</span><br></pre></td></tr></table></figure>



<h3 id="4-10-删除命令"><a href="#4-10-删除命令" class="headerlink" title="4.10 删除命令"></a>4.10 删除命令</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...]</span><br><span class="line">注意:如果删除文件夹需要加-r</span><br><span class="line"></span><br><span class="line">[-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...]</span><br><span class="line">注意:必须是空文件夹,如果非空必须使用rm删除</span><br></pre></td></tr></table></figure>



<h3 id="4-11-查看磁盘利用率和文件大小"><a href="#4-11-查看磁盘利用率和文件大小" class="headerlink" title="4.11 查看磁盘利用率和文件大小"></a>4.11 查看磁盘利用率和文件大小</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[-df [-h] [&lt;path&gt; ...]] 查看分布式系统的磁盘使用情况</span><br><span class="line">[-du [-s] [-h] &lt;path&gt; ...]	#查看分布式系统上当前路径下文件的情况	-h：human 以人类可读的方式显示</span><br></pre></td></tr></table></figure>



<h3 id="4-12-修改权限的"><a href="#4-12-修改权限的" class="headerlink" title="4.12 修改权限的"></a>4.12 修改权限的</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">跟本地的操作一致,-R是让子目录或文件也进行相应的修改</span><br><span class="line">[-chgrp [-R] GROUP PATH...]</span><br><span class="line">[-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...]</span><br><span class="line">[-chown [-R] [OWNER][:[GROUP]] PATH...]</span><br></pre></td></tr></table></figure>



<h3 id="4-13-修改文件的副本数"><a href="#4-13-修改文件的副本数" class="headerlink" title="4.13 修改文件的副本数"></a>4.13 修改文件的副本数</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...]</span><br><span class="line">调用格式:hadoop fs -setrep  3 /   将hdfs根目录及子目录下的内容设置成3个副本</span><br><span class="line">注意:当设置的副本数量与初始化时默认的副本数量不一致时,集群会作出反应,比原来多了会自动进行复制.</span><br></pre></td></tr></table></figure>



<h3 id="4-14-查看文件的状态"><a href="#4-14-查看文件的状态" class="headerlink" title="4.14 查看文件的状态"></a>4.14 查看文件的状态</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs [generic options] -stat [format] &lt;path&gt; ...</span><br><span class="line">命令的作用:当向hdfs上写文件时，可以通过dfs.blocksize配置项来设置文件的block的大小。这就导致了hdfs上的不同的文件block的大小是不相同的。有时候想知道hdfs上某个文件的block大小，可以预先估算一下计算的task的个数。stat的意义：可以查看文件的一些属性。</span><br><span class="line">调用格式:hdfs dfs -stat [format] 文件路径</span><br><span class="line">format的形式：</span><br><span class="line">%b：打印文件的大小（目录大小为0）</span><br><span class="line">%n：打印文件名</span><br><span class="line">%o：打印block的size</span><br><span class="line">%r：打印副本数</span><br><span class="line">%y：utc时间 yyyy-MM-dd HH:mm:ss</span><br><span class="line">%Y：打印自1970年1月1日以来的utc的微秒数</span><br><span class="line">%F：目录打印directory，文件打印regular file</span><br><span class="line"></span><br><span class="line">注意:</span><br><span class="line">1)当使用-stat命令但不指定format时，只打印创建时间，相当于%y</span><br><span class="line">2)-stat 后面只跟目录,%r,%o等打印的都是0,只有文件才有副本和大小</span><br></pre></td></tr></table></figure>



<h3 id="4-15-测试"><a href="#4-15-测试" class="headerlink" title="4.15 测试"></a>4.15 测试</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs [generic options] -test -[defsz] &lt;path&gt;    </span><br><span class="line">参数说明: -e:文件是否存在  存在返回0    -z:文件是否为空  为空返回0   -d:是否是路径(目录) ,是返回0</span><br><span class="line">调用格式:hdfs dfs -test -d 文件 </span><br><span class="line">实例:hdfs dfs -test -d /shelldata/111.txt  &amp;&amp; echo &quot;OK&quot;  || echo &quot;no&quot;</span><br><span class="line">解释:测试当前的内容是否是文件夹 ,如果是返回ok,如果不是返回no</span><br></pre></td></tr></table></figure>



<h2 id="第五章-HDFS的块的概念"><a href="#第五章-HDFS的块的概念" class="headerlink" title="第五章 HDFS的块的概念"></a>第五章 HDFS的块的概念</h2><h3 id="5-1-传统型分布式文件系统的缺点"><a href="#5-1-传统型分布式文件系统的缺点" class="headerlink" title="5.1 传统型分布式文件系统的缺点"></a>5.1 传统型分布式文件系统的缺点</h3><p>现在想象一下这种情况：有四个文件 0.5TB的file1，1.2TB的file2，50GB的file3，100GB的file4；有7个服务器，每个服务器上有10个1TB的硬盘。</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401105627870.png" alt="image-20210401105627870"></p>
<p>在存储方式上，我们可以将这四个文件存储在同一个服务器上（当然大于1TB的文件需要切分）。那么缺点也就暴露了出来：</p>
<p>第一、负载不均衡。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因为文件大小不一致，势必会导致有的节点磁盘的利用率高，有的节点磁盘利用率低。</span><br></pre></td></tr></table></figure>

<p>第二、网络瓶颈问题。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">一个过大的文件存储在一个节点磁盘上，当有并行处理时，每个线程都需要从这个节点磁盘上读取这个文件的内容，那么就会出现网络瓶颈，不利于分布式的数据处理。</span><br></pre></td></tr></table></figure>



<h3 id="5-2-HDFS的块"><a href="#5-2-HDFS的块" class="headerlink" title="5.2 HDFS的块"></a>5.2 HDFS的块</h3><p>HDFS与其他普通文件系统一样，同样引入了块(Block)的概念，并且<strong>块的大小是固定的</strong>。但是不像普通文件系统那样小，而是根据实际需求可以自定义的。块是HDFS系统当中的最小存储单位，在hadoop2.0中默认大小为128MB（hadoop1.x中的块大小为64M）。在HDFS上的文件会被拆分成多个块，每个块作为独立的单元进行存储。多个块存放在不同的DataNode上，<strong>整个过程中 HDFS系统会保证一个块存储在一个数据节点上</strong> 。但值得注意的是，如果某文件大小或者文件的最后一个块没有到达128M，<strong>则不会占据整个块空间</strong> 。</p>
<p>我们来看看HDFS的设计思想：以下图为例，来进行解释。</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401105639009.png" alt="image-20210401105639009"></p>
<h3 id="5-3-HDFS的块大小"><a href="#5-3-HDFS的块大小" class="headerlink" title="5.3 HDFS的块大小"></a>5.3 HDFS的块大小</h3><p>HDFS上的块大小为什么会远远大于传统文件?  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 目的是为了最小化寻址开销时间。</span><br><span class="line">	 在I/O开销中，机械硬盘的寻址时间是最耗时的部分，一旦找到第一条记录，剩下的顺序读取效率是非常高的，因此以块为单位读写数据，可以尽量减少总的磁盘寻道时间。  </span><br><span class="line">	 HDFS寻址开销不仅包括磁盘寻道开销，还包括数据块的定位开销，当客户端需要访问一个文件时，首先从名称节点获取组成这个文件的数据块的位置列表，然后根据位置列表获取实际存储各个数据块的数据节点的位置，最后，数据节点根据数据块信息在本地Linux文件系统中找到对应的文件，并把数据返回给客户端，设计成一个比较大的块，可以减少每个块儿中数据的总的寻址开销，相对降低了单位数据的寻址开销</span><br><span class="line">	 磁盘的寻址时间为大约在5~15ms之间，平均值为10ms,而最小化寻址开销时间普遍认为占1秒的百分之一是最优的，那么块大小的选择就参考1秒钟的传输速度，比如2010年硬盘的传输速率是100M/s，那么就选择块大小为128M。</span><br><span class="line"></span><br><span class="line">2. 为了节省内存的使用率</span><br><span class="line">	 一个块的元数据大约150个字节。1亿个块，不论大小，都会占用20G左右的内存。因此块越大，集群相对存储的数据就越多。所以暴漏了HDFS的一个缺点，不适合存储小文件。</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">不适合存储小文件解释:</span><br><span class="line">1. 从存储能力出发（固定内存）</span><br><span class="line">   因为HDFS的文件是以块为单位存储的，且如果文件大小不到128M的时候，是不会占用整个块的空间的。但是，这个块依然会在内存中占用150个字节的元数据。因此，同样的内存占用的情况下，大量的小文件会导致集群的存储能力不足。</span><br><span class="line">   例如: 同样是128G的内存，最多可存储9.2亿个块。如果都是小文件，例如1M，则集群存储的数据大小为9.2亿*1M = 877TB的数据。但是如果存储的都是128M的文件，则集群存储的数据大小为109.6PB的数据。存储能力大不相同。</span><br><span class="line">   </span><br><span class="line">2. 从内存占用出发（固定存储能力）</span><br><span class="line">   同样假设存储1M和128M的文件对比，同样存储1PB的数据，如果是1M的小文件存储，占用的内存空间为1PB/1Mb*150Byte = 150G的内存。如果存储的是128M的文件存储，占用的内存空间为1PB/128M*150Byte = 1.17G的内存占用。可以看到，同样存储1PB的数据，小文件的存储比起大文件占用更多的内存。</span><br></pre></td></tr></table></figure>



<h3 id="5-4-块的相关参数设置"><a href="#5-4-块的相关参数设置" class="headerlink" title="5.4 块的相关参数设置"></a>5.4 块的相关参数设置</h3><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">当然块大小在默认配置文件hdfs-default.xml中有相关配置，我们可以在hdfs-site.xml中进行重置</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>默认块大小，以字节为单位。可以使用以下后缀(不区分大小写):k，m，g，t，p，e以重新指定大小(例如128k, 512m, 1g等)<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.fs-limits.min-block-size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>以字节为单位的最小块大小，由Namenode在创建时强制执行时间。这可以防止意外创建带有小块的文件降低性能。<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.fs-limits.max-blocks-per-file<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1048576<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>每个文件的最大块数，由写入时的Namenode执行。这可以防止创建降低性能的超大文件<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>



<h3 id="5-5-块的存储位置"><a href="#5-5-块的存储位置" class="headerlink" title="5.5 块的存储位置"></a>5.5 块的存储位置</h3><p>在<code>hdfs-site.xml</code>中我们配置过下面这个属性，这个属性的值就是块在linux系统上的存储位置</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 确定DFS数据节点应该将其块存储在本地文件系统的何处--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401105654266.png" alt="image-20210401105654266"></p>
<h3 id="5-6-HDFS的优点"><a href="#5-6-HDFS的优点" class="headerlink" title="5.6 HDFS的优点"></a>5.6 HDFS的优点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. 高容错性（硬件故障是常态）：数据自动保存多个副本，副本丢失后，会自动恢复</span><br><span class="line">2. 适合大数据集：GB、TB、甚至PB级数据、千万规模以上的文件数量，1000以上节点规模。</span><br><span class="line">3. 数据访问： 一次性写入，多次读取；保证数据一致性,安全性</span><br><span class="line">4. 构建成本低：可以构建在廉价机器上。</span><br><span class="line">5. 多种软硬件平台中的可移植性 </span><br><span class="line">6. 高效性：Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。</span><br><span class="line">7. 高可靠性：Hadoop的存储和处理数据的能力值得人们信赖.</span><br></pre></td></tr></table></figure>



<h3 id="5-7-HDFS的缺点"><a href="#5-7-HDFS的缺点" class="headerlink" title="5.7 HDFS的缺点"></a>5.7 HDFS的缺点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 不适合做低延迟数据访问：</span><br><span class="line">	 HDFS的设计目标有一点是：处理大型数据集，高吞吐率。这一点势必要以高延迟为代价的。因此HDFS不适合处理用户要求的毫秒级的低延迟应用请求</span><br><span class="line">2. 不适合小文件存取：</span><br><span class="line">	 一个是大量小文件需要消耗大量的寻址时间，违反了HDFS的尽可能减少寻址时间比例的设计目标。第二个是内存有限，一个block元数据大内存消耗大约为150个字节，存储一亿个block和存储一亿个小文件都会消耗20G内存。因此相对来说，大文件更省内存。</span><br><span class="line">3. 不适合并发写入，文件随机修改：</span><br><span class="line">	 HDFS上的文件只能拥有一个写者，仅仅支持append操作。不支持多用户对同一个文件的写操作，以及在文件任意位置进行修改</span><br></pre></td></tr></table></figure>



<h2 id="第六章-HDFS的体系结构"><a href="#第六章-HDFS的体系结构" class="headerlink" title="第六章 HDFS的体系结构"></a>第六章 HDFS的体系结构</h2><h3 id="6-1-体系结构解析"><a href="#6-1-体系结构解析" class="headerlink" title="6.1 体系结构解析"></a>6.1 体系结构解析</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">HDFS采用的是master/slaves这种主从的结构模型来管理数据，这种结构模型主要由四个部分组成，分别是Client(客户端)、Namenode(名称节点)、Datanode(数据节点)和SecondaryNameNode。</span><br><span class="line"></span><br><span class="line">真正的一个HDFS集群包括一个Namenode和若干数目的Datanode。</span><br><span class="line"></span><br><span class="line">Namenode是一个中心服务器，负责管理文件系统的命名空间 (Namespace),它在内存中维护着命名空间的最新状态，同时并持久性文件（fsimage和edit）进行备份，防止宕机后，数据丢失。namenode还负责管理客户端对文件的访问，比如权限验证等。</span><br><span class="line"></span><br><span class="line">集群中的Datanode一般是一个节点运行一个Datanode进程，真正负责管理客户端的读写请求，在Namenode的统一调度下进行数据块的创建、删除和复制等操作。数据块实际上都是保存在Datanode本地的Linux文件系统中的。每个Datanode会定期的向Namenode发送数据，报告自己的状态(我们称之为心跳机制)。没有按时发送心跳信息的Datanode会被Namenode标记为“宕机”，不会再给他分配任何I/O请求。 </span><br><span class="line"></span><br><span class="line">用户在使用Client进行I/O操作时,仍然可以像使用普通文件系统那样，使用文件名去存储和访问文件，只不过，在HDFS内部，一个文件会被切分成若干个数据块，然后被分布存储在若干个Datanode上。</span><br><span class="line"></span><br><span class="line">比如，用户在Client上需要访问一个文件时，HDFS的实际工作流程如此：客户端先把文件名发送给Namenode，Namenode根据文件名找到对应的数据块信息及其每个数据块所在的Datanode位置，然后把这些信息发送给客户端。之后，客户端就直接与这些Datanode进行通信，来获取数据（这个过程，Namenode并不参与数据块的传输）。这种设计方式，实现了并发访问，大大提高了数据的访问速度。</span><br><span class="line"></span><br><span class="line">HDFS集群中只有唯一的一个Namenode,负责所有元数据的管理工作。这种方式保证了Datanode不会脱离Namenode的控制，同时，用户数据也永远不会经过Namenode，大大减轻了Namenode的工作负担，使之更方便管理工作。通常在部署集群中，我们要选择一台性能较好的机器来作为Namenode。当然，一台机器上也可以运行多个Datanode，甚至Namenode和Datanode也可以在一台机器上，只不过实际部署中，通常不会这么做的</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110221175.png" alt="image-20210401110221175"></p>
<h3 id="6-2-开机启动HDFS的过程"><a href="#6-2-开机启动HDFS的过程" class="headerlink" title="6.2 开机启动HDFS的过程"></a>6.2 开机启动HDFS的过程</h3><h4 id="6-2-1-非第一次启动集群"><a href="#6-2-1-非第一次启动集群" class="headerlink" title="6.2.1 非第一次启动集群"></a>6.2.1 非第一次启动集群</h4><p>我们应该知道，在启动namenode之前，内存里是没有任何有关于元数据的信息的。那么启动集群的过程是怎样的呢？下面来叙述一下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">第一步：</span><br><span class="line">Namenode在启动时，会先加载name目录下最近的fsimage文件.</span><br><span class="line">将fsimage里保存的元数据加载到内存当中，这样内存里就有了之前检查点里存储的所有元数据。但是还少了从最近一次检查时间点到关闭系统时的部分数据，也就是edit日志文件里存储的数据。</span><br><span class="line"></span><br><span class="line">第二步：</span><br><span class="line">加载剩下的edit日志文件</span><br><span class="line">将从最近一次检查点到目前为止的所有的日志文件加载到内存里，重演一次客户端的操作，这样，内存里就是最新的文件系统的所有元数据了。</span><br><span class="line"></span><br><span class="line">第三步：</span><br><span class="line">进行检查点设置（满足条件会进行）</span><br><span class="line">namenode会终止之前正在使用的edit文件,创建一个空的edit日志文件。然后将所有的未合并过的edit日志文件和fsimage文件进行合并，产生一个新的fsimage.</span><br><span class="line"></span><br><span class="line">第四步：</span><br><span class="line">处于安全模式下，等待datanode节点的心跳反馈，当收到99.9%的块的至少一个副本后，退出安全模式，开始转为正常状态。</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110453904.png" alt="image-20210401110453904"></p>
<h4 id="6-2-2-第一次启动集群"><a href="#6-2-2-第一次启动集群" class="headerlink" title="6.2.2 第一次启动集群"></a>6.2.2 第一次启动集群</h4><p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110508586.png" alt="image-20210401110508586"></p>
<h3 id="6-3-SecondaryNameNode的工作机制"><a href="#6-3-SecondaryNameNode的工作机制" class="headerlink" title="6.3 SecondaryNameNode的工作机制"></a>6.3 SecondaryNameNode的工作机制</h3><p>SecondaryNamenode，是HDFS集群中的重要组成部分，它可以辅助Namenode进行fsimage和editlog的合并工作，减小editlog文件大小，以便缩短下次Namenode的重启时间，能尽快退出安全模式。</p>
<p>两个文件的合并周期，称之为检查点机制（checkpoint），是可以通过hdfs-default.xml配置文件进行修改的：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>3600<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>两次检查点间隔的秒数，默认是1个小时<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>		 </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.txns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1000000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>txid执行的次数达到100w次，也执行checkpoint<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span>		 </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.check.period<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>60<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>60秒一检查txid的执行次数<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110748741.png" alt="image-20210401110748741"></p>
<p>通过上图，可以总结如下:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. SecondaryNamenode请求Namenode停止使用正在编辑的editlog文件，Namenode会创建新的editlog文件(小了吧)，同时更新seed_txid文件。</span><br><span class="line">2. SecondaryNamenode通过HTTP协议获取Namenode上的fsimage和editlog文件。</span><br><span class="line">3. SecondaryNamenode将fsimage读进内存当中，并逐步分析editlog文件里的数据，进行合并操作，然后写入新文件fsimage_x.ckpt文件中。</span><br><span class="line">4. SecondaryNamenode将新文件fsimage_x.ckpt通过HTTP协议发送回Namenode。</span><br><span class="line">5. Namenode再进行更名操作。</span><br></pre></td></tr></table></figure>



<h2 id="第七章-HDFS的读写流程"><a href="#第七章-HDFS的读写流程" class="headerlink" title="第七章 HDFS的读写流程"></a>第七章 HDFS的读写流程</h2><h3 id="7-1-读流程详解"><a href="#7-1-读流程详解" class="headerlink" title="7.1 读流程详解"></a>7.1 读流程详解</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">读操作：  </span><br><span class="line">	- hdfs dfs -get /file02 ./file02</span><br><span class="line">	- hdfs  dfs -copyToLocal  /file02 ./file02</span><br><span class="line">	- FSDataInputStream fsis = fs.open(&quot;/input/a.txt&quot;);</span><br><span class="line">	- fsis.read(byte[] a)</span><br><span class="line">	- fs.copyToLocal(path1,path2)	</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401110919469.png" alt="image-20210401110919469"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 客户端通过调用FileSystem对象的open()方法来打开希望读取的文件，对于HDFS来说，这个对象是DistributedFileSystem，它通过使用远程过程调用(RPC)来调用namenode,以确定文件起始块的位置</span><br><span class="line"></span><br><span class="line">2. 对于每一个块,NameNode返回存有该块副本的DataNode地址,并根据距离客户端的远近来排序。</span><br><span class="line"></span><br><span class="line">3. DistributedFileSystem实例会返回一个FSDataInputStream对象（支持文件定位功能）给客户端以便读取数据，接着客户端对这个输入流调用read()方法</span><br><span class="line"></span><br><span class="line">4. FSDataInputStream随即连接距离最近的文件中第一个块所在的DataNode,通过对数据流反复调用read()方法，可以将数据从DataNode传输到客户端</span><br><span class="line"></span><br><span class="line">5. 当读取到块的末端时，FSInputStream关闭与该DataNode的连接，然后寻找下一个块的最佳DataNode</span><br><span class="line"></span><br><span class="line">6. 客户端从流中读取数据时，块是按照打开FSInputStream与DataNode的新建连接的顺序读取的。它也会根据需要询问NameNode来检索下一批数据块的DataNode的位置。一旦客户端完成读取，就对FSInputStream调用close方法</span><br><span class="line"></span><br><span class="line">注意：在读取数据的时候，如果FSInputStream与DataNode通信时遇到错误，会尝试从这个块的最近的DataNode读取数据，并且记住那个故障的DataNode,保证后续不会反复读取该节点上后续的块。FInputStream也会通过校验和确认从DataNode发来的数据是否完整。如果发现有损坏的块，FSInputStream会从其他的块读取副本，并且将损坏的块通知给NameNode</span><br></pre></td></tr></table></figure>



<h3 id="7-2-写流程的详解"><a href="#7-2-写流程的详解" class="headerlink" title="7.2 写流程的详解"></a>7.2 写流程的详解</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">写操作： </span><br><span class="line">	- hdfs dfs -put ./file02 /file02</span><br><span class="line">	- hdfs  dfs -copyFromLocal  ./file02 /file02</span><br><span class="line">	- FSDataOutputStream fsout = fs.create(path)；fsout.write(byte[])</span><br><span class="line">	- fs.copyFromLocal(path1,path2)</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401111009995.png" alt="image-20210401111009995"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. 客户端通过对DistributedFileSystem对象调用create()方法来新建文件</span><br><span class="line"></span><br><span class="line">2. DistributedFileSystem对namenode创建一个RPC调用，在文件系统的命名空间中新建一个文件，此时该文件中还没有相应的数据块</span><br><span class="line"></span><br><span class="line">3. namenode执行各种不同的检查，以确保这个文件不存在以及客户端有新建该文件的权限。如果检查通过，namenode就会为创建新文件记录一条事务记录(否则，文件创建失败并向客户端抛出一个IOException异常)。DistributedFileSystem向客户端返回一个FSDataOuputStream对象，由此客户端可以开始写入数据，</span><br><span class="line"></span><br><span class="line">4. 在客户端写入数据时，FSOutputStream将它分成一个个的数据包(packet)，并写入一个内部队列，这个队列称为“数据队列”（data queue）。DataStreamer线程负责处理数据队列，它的责任是挑选出合适存储数据复本的一组datanode，并以此来要求namenode分配新的数据块。这一组datanode将构成一个管道，以默认复本3个为例，所以该管道中有3个节点.DataStreamer将数据包流式传输到管道中第一个datanode，该datanode存储数据包并将它发送到管道中的第2个datanode，同样，第2个datanode存储该数据包并且发送给管道中的第三个datanode。DataStreamer在将一个个packet流式传输到第一个Datanode节点后，还会将此packet从数据队列移动到另一个队列确认队列(ack queue)中。</span><br><span class="line"></span><br><span class="line">5. datanode写入数据成功之后，会为ResponseProcessor线程发送一个写入成功的信息回执，当收到管道中所有的datanode确认信息后，ResponseProcessoer线程会将该数据包从确认队列中删除。</span><br></pre></td></tr></table></figure>

<p>如果任何datanode在写入数据期间发生故障，则执行以下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 首先关闭管道，把确认队列中的所有数据包都添加回数据队列的最前端，以确保故障节点下游的datanode不会漏掉任何一个数据包</span><br><span class="line">2. 为存储在另一正常datanode的当前数据块制定一个新标识，并将该标识传送给namenode，以便故障datanode在恢复后可以删除存储的部分数据块</span><br><span class="line">3. 从管道中删除故障datanode，基于两个正常datanode构建一条新管道，余下数据块写入管道中正常的datanode</span><br><span class="line">4. namenode注意到块复本不足时，会在一个新的Datanode节点上创建一个新的复本。</span><br><span class="line"></span><br><span class="line">注意：在一个块被写入期间可能会有多个datanode同时发生故障，但概率非常低。只要写入了dfs.namenode.replication.min的复本数（默认1），写操作就会成功，并且这个块可以在集群中异步复制，直到达到其目标复本数dfs.replication的数量（默认3）</span><br></pre></td></tr></table></figure>



<h2 id="第八章-Zookeeper的概述"><a href="#第八章-Zookeeper的概述" class="headerlink" title="第八章 Zookeeper的概述"></a>第八章 Zookeeper的概述</h2><h3 id="8-1-Zookeeper是什么"><a href="#8-1-Zookeeper是什么" class="headerlink" title="8.1 Zookeeper是什么"></a>8.1 Zookeeper是什么</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">1. zookeeper是一个为分布式应用程序提供的一个分布式开源协调服务框架。是Google的Chubby的一个开源实现，是Hadoop和Hbase的重要组件。主要用于解决分布式集群中应用系统的一致性问题。</span><br><span class="line">2. 提供了基于类似Unix系统的目录节点树方式的数据存储。</span><br><span class="line">3. 可用于维护和监控存储的数据的状态的变化，通过监控这些数据状态的变化，从而达到基于数据的集群管理</span><br><span class="line">4. 提供了一组原语(机器指令)，提供了java和c语言的接口</span><br></pre></td></tr></table></figure>



<h3 id="8-2-Zookeeper的特点"><a href="#8-2-Zookeeper的特点" class="headerlink" title="8.2 Zookeeper的特点"></a>8.2 Zookeeper的特点</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 也是一个分布式集群，一个领导者(leader),多个跟随者(follower).</span><br><span class="line">2. 集群中只要有半数以上的节点存活，Zookeeper集群就能正常服务。</span><br><span class="line">3. 全局数据一致性：每个server保存一份相同的数据副本，client无论连接到哪个server,数据都是一致的。</span><br><span class="line">4. 更新请求按顺序进行：来自同一个client的更新请求按其发送顺序依次执行</span><br><span class="line">5. 数据更新的原子性：一次数据的更新要么成功，要么失败</span><br><span class="line">6. 数据的实时性：在一定时间范围内，client能读到最新数据。</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401111303037.png" alt="image-20210401111303037"></p>
<h3 id="8-3-Zookeeper的数据模型"><a href="#8-3-Zookeeper的数据模型" class="headerlink" title="8.3 Zookeeper的数据模型"></a>8.3 Zookeeper的数据模型</h3><p>Zookeeper的数据模型采用的与Unix文件系统类似的层次化的树形结构。我们可以将其理解为一个具有高可用特征的文件系统。这个文件系统中没有文件和目录，而是统一使用”节点”(node)的概念，称之为znode。znode既可以作为保存数据的容器(如同文件),也可以作为保存其他znode的容器(如同目录)。所有的znode构成了一个层次化的命名空间。</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401111326294.png" alt="image-20210401111326294"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- Zookeeper 被设计用来实现协调服务（这类服务通常使用小数据文件)，而不是用于大容量数据存储，因此一个znode能存储的数据被限制在1MB以内，</span><br><span class="line">- 每个znode都可以通过其路径唯一标识。</span><br></pre></td></tr></table></figure>



<h3 id="8-4-Zookeeper的应用场景"><a href="#8-4-Zookeeper的应用场景" class="headerlink" title="8.4 Zookeeper的应用场景"></a>8.4 Zookeeper的应用场景</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 统一配置管理</span><br><span class="line">2. 统一集群管理</span><br><span class="line">3. 服务器节点动态上下线感知</span><br><span class="line">4. 软负载均衡等</span><br><span class="line">5. 分布式锁</span><br><span class="line">6. 分布式队列</span><br></pre></td></tr></table></figure>



<h2 id="第九章-Zookeeper的安装"><a href="#第九章-Zookeeper的安装" class="headerlink" title="第九章 Zookeeper的安装"></a>第九章 Zookeeper的安装</h2><h3 id="9-1-安装与环境变量的配置"><a href="#9-1-安装与环境变量的配置" class="headerlink" title="9.1. 安装与环境变量的配置"></a>9.1. 安装与环境变量的配置</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1. 将zookeeper-3.4.10.tar.gz上传到/root中</span><br><span class="line">2. 解压</span><br><span class="line">   [root@qianfeng01 ~]# tar -zxvf zookeeper-3.4.10.tar.gz -C /opt/apps/</span><br><span class="line">3. 更名zookeeper</span><br><span class="line">   [root@qianfeng01 ~]# cd /opt/apps/</span><br><span class="line">   [root@qianfeng01 local]# mv zookeeper-3.4.10 zookeeper</span><br><span class="line">4. 配置环境变量</span><br><span class="line">   [root@qianfeng01 local]# vi  /etc/profile</span><br><span class="line">   .........省略......</span><br><span class="line">   export ZOOKEEPER_HOME=/opt/apps/zookeeper</span><br><span class="line">   export PATH=$ZOOKEEPER_HOME/bin:$PATH</span><br><span class="line">5. 使当前会话生效</span><br><span class="line">   [root@qianfeng01 local]# source /etc/profile</span><br><span class="line">6. 检查如下：</span><br><span class="line">如果只检查环境变量是否配置成功，只需要使用tab键进行补全zk，是否zookeeper的相关脚本提示即可。</span><br></pre></td></tr></table></figure>



<h3 id="9-2-集群模式的配置"><a href="#9-2-集群模式的配置" class="headerlink" title="9.2. 集群模式的配置"></a>9.2. 集群模式的配置</h3><h4 id="9-2-1-Zookeeper的服务进程布局"><a href="#9-2-1-Zookeeper的服务进程布局" class="headerlink" title="9.2.1 Zookeeper的服务进程布局"></a>9.2.1 Zookeeper的服务进程布局</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">qianfeng01		QuorumPeerMain</span><br><span class="line">qianfeng02		QuorumPeerMain</span><br><span class="line">qianfeng03		QuorumPeerMain</span><br></pre></td></tr></table></figure>



<h4 id="9-2-2-修改zoo-cfg文件"><a href="#9-2-2-修改zoo-cfg文件" class="headerlink" title="9.2.2 修改zoo.cfg文件"></a>9.2.2 修改zoo.cfg文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 local]# cd ./zookeeper/conf/               </span><br><span class="line">[root@qianfeng01 conf]# cp  zoo_sample.cfg  zoo.cfg   #复制出zoo.cfg文件</span><br><span class="line">[root@qianfeng01 conf]# vi zoo.cfg</span><br><span class="line">tickTime=2000				# 定义的时间单元(单位毫秒)，下面的两个值都是tickTime的倍数。</span><br><span class="line">initLimit=10				# follower连接并同步leader的初始化连接时间。</span><br><span class="line">syncLimit=5					# 心跳机制的时间(正常情况下的请求和应答的时间)</span><br><span class="line">dataDir=/usr/local/zookeeper/zkData       # 修改zookeeper的存储路径，zkData目录一会要创建出来</span><br><span class="line">clientPort=2181							 							# 客户端连接服务器的port</span><br><span class="line">server.1=qianfeng01:2888:3888    			 		# 添加三个服务器节点</span><br><span class="line">server.2=qianfeng02:2888:3888</span><br><span class="line">server.3=qianfeng03:2888:3888</span><br><span class="line"></span><br><span class="line">解析Server.id=ip:port1:port2</span><br><span class="line">id:		服务器的id号，对应zkData/myid文件内的数字</span><br><span class="line">ip: 	服务器的ip地址</span><br><span class="line">port1:	follower与leader交互的port</span><br><span class="line">port2:	选举期间使用的port</span><br><span class="line"></span><br><span class="line">注意：此配置文件中，不支持汉字注释</span><br></pre></td></tr></table></figure>



<h4 id="9-2-3-添加myid"><a href="#9-2-3-添加myid" class="headerlink" title="9.2.3 添加myid"></a>9.2.3 添加myid</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在<span class="variable">$ZOOKEEPER_HOME</span>/zkData目录下添加myid文件，内容为server的<span class="built_in">id</span>号</span></span><br><span class="line">[root@qianfeng01 conf]# cd ..</span><br><span class="line">[root@qianfeng01 zookeeper]# mkdir zkData</span><br><span class="line">[root@qianfeng01 zookeeper]# cd zkData</span><br><span class="line">[root@qianfeng01 zkData]# echo &quot;1&quot; &gt;&gt; myid</span><br></pre></td></tr></table></figure>



<h4 id="9-2-4-搭建其他两个server节点的环境"><a href="#9-2-4-搭建其他两个server节点的环境" class="headerlink" title="9.2.4 搭建其他两个server节点的环境"></a>9.2.4 搭建其他两个server节点的环境</h4><p><strong>1）</strong>使用scp命令将zookeeper环境 复制到qianfeng02和qianfeng03中</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 zkData]<span class="comment"># cd /usr/local</span></span><br><span class="line">[root@qianfeng01 apps]<span class="comment"># scp -r zookeeper qianfeng02:/usr/local</span></span><br><span class="line">[root@qianfeng01 apps<span class="comment"># scp -r zookeeper qianfeng03:/usr/local</span></span><br></pre></td></tr></table></figure>

<p><strong>2）</strong> 使用scp命令拷贝/etc/profile到两台机器上(别忘记source一下)</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 apps]<span class="comment"># scp /etc/profile qianfeng02:/etc/ 	</span></span><br><span class="line">[root@qianfeng01 apps]<span class="comment"># scp /etc/profile qianfeng03:/etc/</span></span><br></pre></td></tr></table></figure>

<p><strong>3）</strong> 修改qianfeng02的myid文件的内容为2</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]<span class="comment"># ssh qianfeng02</span></span><br><span class="line">[root@qianfeng02 ~]<span class="comment"># echo &quot;2&quot; &gt; /opt/apps/zookeeper/zkData/myid</span></span><br></pre></td></tr></table></figure>

<p><strong>4）</strong>  修改qianfeng03的myid文件的内容为3</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng02 ~]<span class="comment"># ssh qianfeng03</span></span><br><span class="line">[root@qianfeng03 ~]<span class="comment"># echo &quot;3&quot; &gt; /opt/apps/zookeeper/zkData/myid</span></span><br></pre></td></tr></table></figure>



<h4 id="9-2-5-启动zookeeper"><a href="#9-2-5-启动zookeeper" class="headerlink" title="9.2.5 启动zookeeper"></a>9.2.5 启动zookeeper</h4><p><strong>1）</strong>三台机器上都启动zookeeper的服务    (注意保证防火墙是关闭的)</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 ~]<span class="comment"># zkServer.sh start</span></span><br><span class="line">再查看一下状态</span><br><span class="line">[root@qianfeng01 ~]<span class="comment"># zkServer.sh status</span></span><br></pre></td></tr></table></figure>

<p><strong>2）</strong> 启动客户端的操作：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">zkCli.sh [-server] [ ip:port]</span><br><span class="line"></span><br><span class="line">reg:</span><br><span class="line">[root@qianfeng01 ~]<span class="comment"># zkCli.sh								#启动客户端，连接本地服务进程 </span></span><br><span class="line">[root@qianfeng01 ~]<span class="comment"># zkCli.sh -server qianfeng02:2181			#启动客户端，连接qianfeng02上的服务进程 </span></span><br></pre></td></tr></table></figure>



<h2 id="第十章-Zookeeper的Shell操作"><a href="#第十章-Zookeeper的Shell操作" class="headerlink" title="第十章 Zookeeper的Shell操作"></a>第十章 Zookeeper的Shell操作</h2><table>
<thead>
<tr>
<th>命令</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>ls</td>
<td>查看某个目录包含的所有文件</td>
<td>ls /</td>
</tr>
<tr>
<td>ls2</td>
<td>查看某个目录包含的所有文件，与ls不同的是它查看到time、version等信息</td>
<td>ls2 /</td>
</tr>
<tr>
<td>create</td>
<td>创建znode，并需要设置初始内容</td>
<td>create /test “test”<br />create -e /test “test”</td>
</tr>
<tr>
<td>get</td>
<td>获取znode的数据</td>
<td>get /test</td>
</tr>
<tr>
<td>set</td>
<td>修改znode的内容</td>
<td>set /test “test2”</td>
</tr>
<tr>
<td>delete</td>
<td>删除znode</td>
<td>delete /test</td>
</tr>
<tr>
<td>quit</td>
<td>退出客户端</td>
<td></td>
</tr>
<tr>
<td>help</td>
<td>帮助命令</td>
<td></td>
</tr>
</tbody></table>
<h2 id="第十一章-YARN的概述"><a href="#第十一章-YARN的概述" class="headerlink" title="第十一章 YARN的概述"></a>第十一章 YARN的概述</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">为克服Hadoop 1.0中HDFS和MapReduce存在的各种问题而提出的，针对Hadoop 1.0中的MapReduce在扩展性和多框架支持方面的不足，提出了全新的资源管理框架YARN.</span><br><span class="line"></span><br><span class="line">Apache YARN（Yet another Resource Negotiator的缩写）是Hadoop集群的资源管理系统，负责为计算程序提供服务器计算资源，相当于一个分布式的操作系统平台，而MapReduce等计算程序则相当于运行于操作系统之上的应用程序。</span><br><span class="line"></span><br><span class="line">yarn被引入Hadoop2,最初是为了改善MapReduce的实现，但是因为具有足够的通用性，同样可以支持其他的分布式计算模式，比如Spark，Tez等计算框架。</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210401112649301.png" alt="image-20210401112649301"></p>
<h2 id="第十二章-YARN的架构及组件"><a href="#第十二章-YARN的架构及组件" class="headerlink" title="第十二章 YARN的架构及组件"></a>第十二章 YARN的架构及组件</h2><h3 id="12-1-MapReduce-1-x的简介"><a href="#12-1-MapReduce-1-x的简介" class="headerlink" title="12.1. MapReduce 1.x的简介"></a>12.1. MapReduce 1.x的简介</h3><p>第一代Hadoop，由分布式存储系统HDFS和分布式计算框架MapReduce组成，其中，HDFS由一个NameNode和多个DataNode组成，MapReduce由一个JobTracker和多个TaskTracker组成，对应Hadoop版本为Hadoop 1.x和0.21.X，0.22.x。</p>
<p><strong>1) MapReduce1的角色</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-1.Client	：作业提交发起者。</span><br><span class="line">-2.JobTracker	：初始化作业，分配作业，与TaskTracker通信，协调整个作业。</span><br><span class="line">-3.TaskTracker ：保持JobTracker通信，在分配的数据片段上执行MapReduce任务。</span><br></pre></td></tr></table></figure>

<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210414093345557.png" alt="image-20210414093345557"></p>
<p><strong>2) MapReduce执行流程</strong></p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210414093407057.png" alt="image-20210414093407057"></p>
<p><strong>步骤1）</strong>提交作业</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">编写MapReduce程序代码,创建job对象，并进行配置，比如输入和输出路径，压缩格式等，然后通过JobClinet来提交作业。</span><br></pre></td></tr></table></figure>

<p><strong>步骤2）</strong>作业的初始化</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">客户端提交完成后，JobTracker会将作业加入队列，然后进行调度，默认的调度方法是FIFO调试方式。</span><br></pre></td></tr></table></figure>

<p><strong>步骤3）</strong>任务的分配</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">TaskTracker和JobTracker之间的通信与任务的分配是通过心跳机制完成的。</span><br><span class="line"></span><br><span class="line">TaskTracker会主动向JobTracker询问是否有作业要做，如果自己可以做，那么就会申请到作业任务，这个任务可以是MapTask也可能是ReduceTask。</span><br></pre></td></tr></table></figure>

<p><strong>步骤4）</strong>任务的执行</p>
<p>申请到任务后，TaskTracker会做如下事情：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-1. 拷贝代码到本地</span><br><span class="line">-2. 拷贝任务的信息到本地</span><br><span class="line">-3. 启动JVM运行任务</span><br></pre></td></tr></table></figure>

<p><strong>步骤5）</strong>状态与任务的更新</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">任务在运行过程中，首先会将自己的状态汇报给TaskTracker，然后由TaskTracker汇总告之JobTracker。任务进度是通过计数器来实现的。</span><br></pre></td></tr></table></figure>

<p><strong>步骤6）</strong> 作业的完成</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">JobTracker是在接受到最后一个任务运行完成后，才会将任务标记为成功。此时会做删除中间结果等善后处理工作。</span><br></pre></td></tr></table></figure>



<h3 id="12-2-YARN的设计思想"><a href="#12-2-YARN的设计思想" class="headerlink" title="12.2. YARN的设计思想"></a>12.2. YARN的设计思想</h3><p>yarn的基本思想是将资源管理和作业调度/监视功能划分为单独的守护进程。其思想是拥有一个全局ResourceManager (RM)，以及每个应用程序拥有一个ApplicationMaster (AM)。应用程序可以是单个作业，也可以是一组作业</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210414095815391.png" alt="image-20210414095815391"></p>
<p>一个ResourceManager和多个NodeManager构成了yarn资源管理框架。他们是yarn启动后长期运行的守护进程，来提供核心服务。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ResourceManager，是在系统中的所有应用程序之间仲裁资源的最终权威，即管理整个集群上的所有资源分配,内部含有一个Scheduler(资源调度器)</span><br><span class="line"></span><br><span class="line">NodeManager，是每台机器的资源管理器，也就是单个节点的管理者，负责启动和监视容器(container)资源使用情况，并向ResourceManager及其 Scheduler报告使用情况</span><br><span class="line"></span><br><span class="line">container:即集群上的可使用资源，包含cpu、内存、磁盘、网络等</span><br><span class="line"></span><br><span class="line">ApplicationMaster（简称AM）实际上是框架的特定的库，每启动一个应用程序，都会启动一个AM，它的任务是与ResourceManager协商资源，并与NodeManager一起执行和监视任务</span><br></pre></td></tr></table></figure>



<p><strong>扩展）</strong>YARN与MapReduce1的比较</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/20191016063209.jpg" alt="img"></p>
<h3 id="12-3-YARN的配置"><a href="#12-3-YARN的配置" class="headerlink" title="12.3. YARN的配置"></a>12.3. YARN的配置</h3><p>yarn属于hadoop的一个组件，不需要再单独安装程序，hadoop中已经存在配置文件的设置，本身就是一个集群，有主节点和从节点。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意&lt;value&gt;&lt;/value&gt;之间的值不能有空格</span><br></pre></td></tr></table></figure>

<p>在mapred-site.xml中的配置如下：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--用于执行MapReduce作业的运行时框架--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--历史任务的内部通讯地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>MapReduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--历史任务的外部监听页面--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>MapReduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>在yarn-site.xml中的配置如下:</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--配置resourcemanager的主机--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置yarn的shuffle服务--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--指定shuffle对应的类 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.MapReduce_shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置resourcemanager的scheduler的内部通讯地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置resoucemanager的资源调度的内部通讯地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置resourcemanager的内部通讯地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置resourcemanager的管理员的内部通讯地址--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--配置resourcemanager的web ui 的监控页面--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">value</span>&gt;</span>qianfeng01:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p><strong>1) 日志位置</strong></p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">jps</span>:<span class="string">当启动进程时出错了解决步骤:可以查看日志</span></span><br><span class="line"></span><br><span class="line"><span class="attr">如果是hdfs上的问题,则查看对应的日志</span></span><br><span class="line"><span class="attr">less</span> <span class="string">或 tail -1000 $HADOOP_HOME/logs/hadoop-&#123;user.name&#125;-&#123;jobname&#125;-&#123;hostname&#125;.log</span></span><br><span class="line"><span class="attr">如果是yarn,则查看</span></span><br><span class="line"><span class="attr">less</span> <span class="string">或 tail -1000 $HADOOP_HOME/logs/yarn-&#123;user.name&#125;-&#123;jobname&#125;-&#123;hostname&#125;.log</span></span><br></pre></td></tr></table></figure>



<p><strong>2) 历史服务</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">如果需要查看YARN的作业历史，需要打开历史服务:</span><br><span class="line"></span><br><span class="line">1. 停止当前的YARN进程</span><br><span class="line">stop-yarn.sh</span><br><span class="line"></span><br><span class="line">2. 在yarn-site.xml中添加配置</span><br><span class="line">&lt;!-- 开启日志聚集功能 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 日志信息保存在文件系统上的最长时间,单位为秒--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;640800&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">3. 分发到其他节点</span><br><span class="line"></span><br><span class="line">4. 启动YARN进程</span><br><span class="line">start-yarn.sh</span><br><span class="line"></span><br><span class="line">5. 开启历史服务</span><br><span class="line">mr-jobhistory-server.sh start historyserver</span><br></pre></td></tr></table></figure>





<h2 id="第十三章-YARN的执行原理"><a href="#第十三章-YARN的执行原理" class="headerlink" title="第十三章 YARN的执行原理"></a>第十三章 YARN的执行原理</h2><p>在MR程序运行时，有五个独立的进程：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-  YarnRunner:用于提交作业的客户端程序</span><br><span class="line">-  ResourceManager:yarn资源管理器，负责协调集群上计算机资源的分配</span><br><span class="line">-  NodeManager:yarn节点管理器，负责启动和监视集群中机器上的计算容器（container）</span><br><span class="line">-  Application Master:负责协调运行MapReduce作业的任务，他和任务都在容器中运行，这些容器由资源管理器分配并由节点管理器进行管理。</span><br><span class="line">-  HDFS:用于共享作业所需文件。</span><br></pre></td></tr></table></figure>

<p>整个过程如下图描述:</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210414100109033.png" alt="image-20210414100109033"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 调用waitForCompletion方法每秒轮询作业的进度，内部封装了submit()方法，用于创建JobCommiter实例，并且调用其的submitJobInternal方法。提交成功后，如果有状态改变，就会把进度报告到控制台。错误也会报告到</span><br><span class="line">控制台</span><br><span class="line">2. JobCommiter实例会向ResourceManager申请一个新应用ID，用于MapReduce作业ID。这期间JobCommiter也会进行检查输出路径的情况，以及计算输入分片。</span><br><span class="line">3. 如果成功申请到ID,就会将运行作业所需要的资源（包括作业jar文件，配置文件和计算所得的输入分片元数据文件）上传到一个用ID命名的目录下的HDFS上。此时副本个数默认是10.</span><br><span class="line">4. 准备工作已经做好，再通知ResourceManager调用submitApplication方法提交作业。</span><br><span class="line">5. ResourceManager调用submitApplication方法后，会通知Yarn调度器（Scheduler），调度器分配一个容器，在节点管理器的管理下在容器中启动 application master进程。</span><br><span class="line">6. application master的主类是MRAppMaster，其主要作用是初始化任务，并接受来自任务的进度和完成报告。</span><br><span class="line">7. 然后从HDFS上接受资源，主要是split。然后为每一个split创建MapTask以及参数指定的ReduceTask，任务ID在此时分配</span><br><span class="line">8. 然后Application Master会向资源管理器请求容器，首先为MapTask申请容器，然后再为ReduceTask申请容器。（5%）</span><br><span class="line">9. 一旦ResourceManager中的调度器（Scheduler），为Task分配了一个特定节点上的容器，Application Master就会与NodeManager进行通信来启动容器。</span><br><span class="line">10. 运行任务是由YarnChild来执行的，运行任务前，先将资源本地化（jar文件，配置文件，缓存文件）</span><br><span class="line">11. 然后开始运行MapTask或ReduceTask。</span><br><span class="line">12. 当收到最后一个任务已经完成的通知后，application master会把作业状态设置为success。然后Job轮询时，知道成功完成，就会通知客户端，并把统计信息输出到控制台</span><br></pre></td></tr></table></figure>



<h2 id="第十四章-YARN的案例测试"><a href="#第十四章-YARN的案例测试" class="headerlink" title="第十四章 YARN的案例测试"></a>第十四章 YARN的案例测试</h2><p><strong>MapReduce:</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@qianfeng01 mapreduce]# hadoop jar hadoop-mapreduce-examples-2.7.6.jar wordcount /input /output</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">INFO client.RMProxy: Connecting to ResourceManager at qianfeng01/192.168.10.101:8032</span><br><span class="line">INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1617775349214_0003</span><br><span class="line">INFO impl.YarnClientImpl: Submitted application application_1617775349214_0003</span><br><span class="line">INFO mapreduce.Job: The url to track the job: http://qianfeng01:8088/proxy/application_1617775349214_0003/</span><br><span class="line">INFO mapreduce.Job: Running job: job_1617775349214_0003</span><br><span class="line">INFO mapreduce.Job: Job job_1617775349214_0003 running in uber mode : false</span><br><span class="line">INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">INFO mapreduce.Job: Job job_1617775349214_0003 completed successfully</span><br><span class="line">INFO mapreduce.Job: Counters: 49</span><br><span class="line">	File System Counters</span><br><span class="line">		FILE: Number of bytes read=111</span><br><span class="line">		FILE: Number of bytes written=245331</span><br><span class="line">		FILE: Number of read operations=0</span><br><span class="line">		FILE: Number of large read operations=0</span><br><span class="line">		FILE: Number of write operations=0</span><br><span class="line">		HDFS: Number of bytes read=218</span><br><span class="line">		HDFS: Number of bytes written=69</span><br><span class="line">		HDFS: Number of read operations=6</span><br><span class="line">		HDFS: Number of large read operations=0</span><br><span class="line">		HDFS: Number of write operations=2</span><br><span class="line">	Job Counters </span><br><span class="line">		Launched map tasks=1</span><br><span class="line">		Launched reduce tasks=1</span><br><span class="line">		Data-local map tasks=1</span><br><span class="line">		Total time spent by all maps in occupied slots (ms)=3359</span><br><span class="line">		Total time spent by all reduces in occupied slots (ms)=3347</span><br><span class="line">		Total time spent by all map tasks (ms)=3359</span><br><span class="line">		Total time spent by all reduce tasks (ms)=3347</span><br><span class="line">		Total vcore-milliseconds taken by all map tasks=3359</span><br><span class="line">		Total vcore-milliseconds taken by all reduce tasks=3347</span><br><span class="line">		Total megabyte-milliseconds taken by all map tasks=3439616</span><br><span class="line">		Total megabyte-milliseconds taken by all reduce tasks=3427328</span><br><span class="line">	Map-Reduce Framework</span><br><span class="line">		Map input records=3</span><br><span class="line">		Map output records=21</span><br><span class="line">		Map output bytes=203</span><br><span class="line">		Map output materialized bytes=111</span><br><span class="line">		Input split bytes=99</span><br><span class="line">		Combine input records=21</span><br><span class="line">		Combine output records=9</span><br><span class="line">		Reduce input groups=9</span><br><span class="line">		Reduce shuffle bytes=111</span><br><span class="line">		Reduce input records=9</span><br><span class="line">		Reduce output records=9</span><br><span class="line">		Spilled Records=18</span><br><span class="line">		Shuffled Maps =1</span><br><span class="line">		Failed Shuffles=0</span><br><span class="line">		Merged Map outputs=1</span><br><span class="line">		GC time elapsed (ms)=126</span><br><span class="line">		CPU time spent (ms)=1250</span><br><span class="line">		Physical memory (bytes) snapshot=451137536</span><br><span class="line">		Virtual memory (bytes) snapshot=4204822528</span><br><span class="line">		Total committed heap usage (bytes)=282591232</span><br><span class="line">	Shuffle Errors</span><br><span class="line">		BAD_ID=0</span><br><span class="line">		CONNECTION=0</span><br><span class="line">		IO_ERROR=0</span><br><span class="line">		WRONG_LENGTH=0</span><br><span class="line">		WRONG_MAP=0</span><br><span class="line">		WRONG_REDUCE=0</span><br><span class="line">	File Input Format Counters </span><br><span class="line">		Bytes Read=119</span><br><span class="line">	File Output Format Counters </span><br><span class="line">		Bytes Written=69</span><br></pre></td></tr></table></figure>



<h2 id="第十五章-YARN的Web-UI查看"><a href="#第十五章-YARN的Web-UI查看" class="headerlink" title="第十五章 YARN的Web UI查看"></a>第十五章 YARN的Web UI查看</h2><p>使用8088端口，可以查看YARN任务的WebUI</p>
<p><img src="http://notebook-1257935960.file.myqcloud.com//img/image-20210414101324462.png" alt="image-20210414101324462"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%A6%82%E5%BF%B5/" rel="tag"># 概念</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2018/09/26/Java/Maven%E4%BD%BF%E7%94%A8%E9%9A%8F%E7%AC%94/" rel="prev" title="maven使用工具">
                  <i class="fa fa-chevron-left"></i> maven使用工具
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2019/09/26/%E5%B7%A5%E5%85%B7/%E4%B8%80%E4%B8%AApython%E7%9A%84%E6%96%87%E4%BB%B6%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%84%9A%E6%9C%AC/" rel="next" title="python的文件服务脚本">
                  python的文件服务脚本 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Guangyu Yang</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a>
  </div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  




  





</body>
</html>
